<html>
<head>
<title>Atividade-Pratica-4.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
.s1 { color: #6aab73;}
.s2 { color: #2aacb8;}
.s3 { color: #cf8e6d;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
Atividade-Pratica-4.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">{</span><span class="s1">&quot;nbformat&quot;</span><span class="s0">:</span><span class="s2">4</span><span class="s0">,</span><span class="s1">&quot;nbformat_minor&quot;</span><span class="s0">:</span><span class="s2">0</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;celltoolbar&quot;</span><span class="s0">:</span><span class="s1">&quot;Edit Metadata&quot;</span><span class="s0">,</span><span class="s1">&quot;kernelspec&quot;</span><span class="s0">:{</span><span class="s1">&quot;display_name&quot;</span><span class="s0">:</span><span class="s1">&quot;Python 3&quot;</span><span class="s0">,</span><span class="s1">&quot;language&quot;</span><span class="s0">:</span><span class="s1">&quot;python&quot;</span><span class="s0">,</span><span class="s1">&quot;name&quot;</span><span class="s0">:</span><span class="s1">&quot;python3&quot;</span><span class="s0">},</span><span class="s1">&quot;language_info&quot;</span><span class="s0">:{</span><span class="s1">&quot;codemirror_mode&quot;</span><span class="s0">:{</span><span class="s1">&quot;name&quot;</span><span class="s0">:</span><span class="s1">&quot;ipython&quot;</span><span class="s0">,</span><span class="s1">&quot;version&quot;</span><span class="s0">:</span><span class="s2">3</span><span class="s0">},</span><span class="s1">&quot;file_extension&quot;</span><span class="s0">:</span><span class="s1">&quot;.py&quot;</span><span class="s0">,</span><span class="s1">&quot;mimetype&quot;</span><span class="s0">:</span><span class="s1">&quot;text/x-python&quot;</span><span class="s0">,</span><span class="s1">&quot;name&quot;</span><span class="s0">:</span><span class="s1">&quot;python&quot;</span><span class="s0">,</span><span class="s1">&quot;nbconvert_exporter&quot;</span><span class="s0">:</span><span class="s1">&quot;python&quot;</span><span class="s0">,</span><span class="s1">&quot;pygments_lexer&quot;</span><span class="s0">:</span><span class="s1">&quot;ipython3&quot;</span><span class="s0">,</span><span class="s1">&quot;version&quot;</span><span class="s0">:</span><span class="s1">&quot;3.7.6&quot;</span><span class="s0">},</span><span class="s1">&quot;colab&quot;</span><span class="s0">:{</span><span class="s1">&quot;name&quot;</span><span class="s0">:</span><span class="s1">&quot;Atividade-Pratica-4.ipynb&quot;</span><span class="s0">,</span><span class="s1">&quot;provenance&quot;</span><span class="s0">:[],</span><span class="s1">&quot;collapsed_sections&quot;</span><span class="s0">:[]},</span><span class="s1">&quot;accelerator&quot;</span><span class="s0">:</span><span class="s1">&quot;GPU&quot;</span><span class="s0">},</span><span class="s1">&quot;cells&quot;</span><span class="s0">:[{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;73h6p7Bmpba4&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# Atividade 04: Implementando uma Rede Convolucional por meio do TensorFlow</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nesta atividade, você irá treinar uma rede convolucional para realizar classificação de imagens utilizando o TensorFlow, e irá testá-la utilizando o dataset CIFAR-10.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nesta atividade, o TensorFlow será examinado em **três diferentes níveis de abstração**, o que irá ajudá-lo a compreender melhor seu funcionamento e prepará-lo para utilizá-lo em outros projetos.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nesta atividade, você irá:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **preparar** o dataset CIFAR-10 para utilizar no treinamento com TensorFlow</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **trabalhar** diretamente sobre grafos de computação do TensorFlow (**Nível de Abstração 1 - Fundamentos do TensorFlow**)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **utilizar** a API `tf.keras.Model` para se definir arquiteturas de redes neurais arbitrárias (**Nível de Abstração 2 - API Keras Model**)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **utilizar** a API `tf.keras.Sequential` para se definir arquiteturas de redes *feed-forward* lineares de forma conveniente e **explorar** biblioteca de funções para construção de modelos mais flexíveis  (**Nível de Abstração 3 - API Keras Sequential + API Funcional**)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Detalhes sobre `Keras` serão apresentados mais adiante nesse *notebook*.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;A tabela abaixo compara as abordagem mencionadas acima em relação ao grau de flexibilidade e de conveniência na implementação de grafos de computação:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;| API           | Flexibilidade | Conveniência |</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;|---------------|-------------|-------------|</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;| Fundamentos      | Alta        | Baixa         |</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;| `tf.keras.Model`     | Alta        | Média      |</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;| `tf.keras.Sequential` | Baixa         | Alta        |&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;i8lJRrWIzQ-k&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;## TensorFlow</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;TensorFlow é um sistema para execução de grafos de computação sobre *tensores*, com um suporte nativo para realização de propagação retrógrada (*backpropagation*). Nele, trabalha-se com *tensores* que representam vetores/matrizes n-dimensionais análogas aos `ndarray` encontrados em `numpy`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Por que utilizar TensorFlow?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;* Seu código irá executar em GPUs! Assim o treinamento pode ser realizado de forma mais rápida. A escrita de código próprio para execução em GPUs pode ser muito trabalhosa e está além do escopo dessa disciplina (porém, o uso de TensorFlow viabiliza a utilização de GPUs de forma transparente).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;* Utilizar com um `framework` como o TensorFlow permite que você produza código de forma mais eficiente e eficaz ao invés de implementar tudo no zero. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;* TensorFlow representa um dos melhores `frameworks` para implementação de redes profundas </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;* Você ganhará familiaridade com o tipo de código utilizado em aprendizagem profunda tanto na academia como na indústria. &quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;PCeTEtLhzQ-l&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;## Como você pode aprender e aprofundar em TensorFlow?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;TensorFlow possui muitos tutoriais disponíveis, incluindo aqueles do próprio pessoal da [Google](https://www.tensorflow.org/get_started/get_started).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;De toda forma, esse *notebook* irá guiá-lo sobre as etapas que você precisa realizar para treinar modelos no TensorFlow. No final desse *notebook*, você encontrará links para tutoriais auxiliares caso você deseje aprender mais ou necessite de esclarecimentos adicionais sobre tópicos que não sejam tratados de forma completa aqui.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**NOTA: Este *notebook* deve ser utilizado com a versão `2.2.0-rc3` do TensorFlow. A maioria dos exemplos na Web atualmente ainda utilizam versões `1.x`, portanto tenha cuidado para não se confundir quando consultar a documentação**.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;## Instalando TensorFlow 2.0 (APENAS SE VOCÊ FOR TRABALHAR LOCALMENTE)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Tenha a última versão de Anaconda instalada em sua máquina.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Crie uma novo ambiente `conda` a partir do `Python 3.7`. Aqui, chamamos esse ambiente de `tf_20_env`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Execute o comando: `source activate tf_20_env`</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;4. Em seguida, use `pip` para instalar TensorFlow 2.0 conforme descrito em: https://www.tensorflow.org/install&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;EPdPJRAizQ-m&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# Preparação</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Primeiramente, vamos carregar os dados do dataset CIFAR-10. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nas atividades anteriores, você utilizou código específico para baixar e ler o dataset CIFAR-10; contudo o pacote `tf.keras.datasets` no TensorFlow fornece funções utilitárias para carga de vários datasets comuns.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Para o propósito dessa atividade, ainda será utilizado código para preprocessamento dos dados e iteração sobre eles em *minibatches*. O pacote `tf.data` no TensorFlow fornece ferramentas para automatizar esse processo, porém trabalhar com esse pacote está fora do escopo dessa atividade. Entretanto, o uso do pacote `tf.data` pode ser muito mais eficiente que a abordagem simples usada nesse *notebook* e você talvez deva considerar seu uso futuramente.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;4_-Rl_rVzQ-n&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;import os</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;import tensorflow as tf</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;import numpy as np</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;import math</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;import timeit</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;import matplotlib.pyplot as plt</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;%matplotlib inline&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;U6q5mAe9zQ-q&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    it for the two-layer neural net classifier. These are the same steps as</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    we used for the SVM, but condensed to a single function.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    cifar10 = tf.keras.datasets.cifar10.load_data()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    (X_train, y_train), (X_test, y_test) = cifar10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_train = np.asarray(X_train, dtype=np.float32)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    y_train = np.asarray(y_train, dtype=np.int32).flatten()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_test = np.asarray(X_test, dtype=np.float32)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    y_test = np.asarray(y_test, dtype=np.int32).flatten()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Subsample the data</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    mask = range(num_training, num_training + num_validation)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_val = X_train[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    y_val = y_train[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    mask = range(num_training)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_train = X_train[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    y_train = y_train[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    mask = range(num_test)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_test = X_test[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    y_test = y_test[mask]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Normalize the data: subtract the mean pixel and divide by std</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_train = (X_train - mean_pixel) / std_pixel</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_val = (X_val - mean_pixel) / std_pixel</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    X_test = (X_test - mean_pixel) / std_pixel</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return X_train, y_train, X_val, y_val, X_test, y_test</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# If there are errors with SSL downloading involving self-signed certificates,</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# it may be that your Python version was recently installed on the current machine.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# See: https://github.com/tensorflow/tensorflow/issues/10779</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# To fix, run the command: /Applications/Python</span><span class="s3">\\ </span><span class="s1">3.7/Install</span><span class="s3">\\ </span><span class="s1">Certificates.command</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;#   ...replacing paths as necessary.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# Invoke the above function to get our data.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;NHW = (0, 1, 2)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Train data shape: ', X_train.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Train labels shape: ', y_train.shape, y_train.dtype)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Validation data shape: ', X_val.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Validation labels shape: ', y_val.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Test data shape: ', X_test.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Test labels shape: ', y_test.shape)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;KY-d8-GuzQ-t&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;class Dataset(object):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def __init__(self, X, y, batch_size, shuffle=False):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        Construct a Dataset object to iterate over data X and labels y</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        - X: Numpy array of data, of any shape</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        - batch_size: Integer giving number of elements per minibatch</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        self.X, self.y = X, y</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        self.batch_size, self.shuffle = batch_size, shuffle</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def __iter__(self):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        N, B = self.X.shape[0], self.batch_size</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        idxs = np.arange(N)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        if self.shuffle:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            np.random.shuffle(idxs)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;test_dset = Dataset(X_test, y_test, batch_size=64)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;AD4pMsGazQ-w&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# We can iterate through a dataset like this:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;for t, (x, y) in enumerate(train_dset):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print(t, x.shape, y.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    if t &gt; 5: break&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;gmi2zkzLzQ-y&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;Você pode opcionalmente **usar GPU bastando setar o *flag* abaixo `USE_GPU` para `True`**.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;## Usuários de Colab</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Caso vocês esteja utilizando o `Colab`, você precisa ativar manualmente o uso de GPU. Você pode fazer isso selecionando `Ambiente de execução -&gt; Alterar tipo de ambiente de execução` (em inglês, `Runtime -&gt; Change runtime type`) e escolhendo `GPU` em `Acelerador de hardware` (em inglês, `Hardware Accelerator`). Observe que você deverá reexecutar as células desde o ínicio (caso faça essa mudança) pois o ambiente será reiniciado uma vez que faça essa alteração.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore-input&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;xAnSyO27zQ-z&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# Set up some global variables</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;USE_GPU = True</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;if USE_GPU:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    device = '/device:GPU:0'</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;else:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    device = '/cpu:0'</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;# Constant to control how often we print when training models</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print_every = 100</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print('Using device: ', device)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;puhqvMrzzQ-2&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# Fundamentos do TensorFlow</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;TensorFlow possui várias APIs de alto nível que o tornam uma ferramenta conveniente para se definir e treinar redes neurais. Algumas dessas construções serão cobertas mais adiante neste *notebook*. Nesta seção, vai se iniciar pela construção de um modelo com as primitivas mais básicas do TensorFlow de forma a ajudá-lo a compreender melhor o que se passa por detrás das APIs de mais alto nível.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**</span><span class="s3">\&quot;</span><span class="s1">Fundamentos do Tensorflow</span><span class="s3">\&quot; </span><span class="s1">são importantes para se compreender os blocos básicos do TensorFlow, porém muitos deles envolvem conceitos do TensorFlow 1.x.** Iremos trabalhar com módulos legados (*antigos*) como `tf.Variable`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Dessa forma, leia com atenção e tente compreender as diferenças entre os módulos TensorFlow legados (antigos - 1.x) e os novos (2.0).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Breve histórico sobre TensorFlow 1.x</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;TensorFlow 1.x é basicamente um `framework` para se trabalhar com **grafos de computação estáticos**. Nós em um grafo de computação representam **tensores** que irão armazenar vetores n-dimensionais quando o grafo estiver em execução; arestas no grafo representam funções que irão operar sobre os tensores quando o grafo for executado de forma a realizar alguma computação.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Antes do TensorFlow 2.0, era necessário se configurar o grafo em duas fases. Existem vários tutoriais que explicam esse processo de duas fases. O processo pode ser resumido da seguinte forma para o TensorFlow 1.x:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. **Construir um grafo de computação que descreve a computação que se deseja realizar**. Esse estágio não realiza nenhuma computação; ele só cria uma representação simbólica da computação a ser realizada. Nesse estágio será(ão) geralmente definido(s) um ou mais `placeholder` que  representam as entradas do grafo de computação.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. **Executar o gafo de computação várias vezes**. Cada vez que o grafo for executado (p.ex. para um passo do método de gradiente), deve-se especificar quais partes do grafo você deseja computar e fornecer um um dicionário `feed_dict` com valores concretos para cada entrada (`placeholder`) do grafo.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Novo paradigma no TensorFlow 2.0</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, com o TensorFlow 2.0, pode-se simplesmente adotar uma forma funcional mais similar os demais códigos escritos em `Python` (e, também, mas similar em espiríto a um outro *framework* muito poderoso denominado `PyTorch`). Você pode obter mais detalhes em https://www.tensorflow.org/guide/eager.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;A principal diferença entre as abordagens do TensorFlow 1.x e do TensorFlow 2.0 é que nesse último não se utiliza `tf.Session`, `tf.run`, `placeholder`, `feed_dict`. Para se obter informações adicionais sobre as diferenças entre as duas versões e como realizar a conversão entre elas, verifique o guia oficial de migração em https://www.tensorflow.org/alpha/guide/migration_guide</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Mais adiante, o foco será sobre essa nova e mais simples abordagem.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;nOyw8AV9zQ-2&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Função Flatten (achatamento)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Uma função de achatamento (`flatten`) pode ser definida de forma a realizar uma reformatação (*reshape*) dos dados de uma imagem para uso em uma rede completamente conectada.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;No TensorFlow, dados para mapas de características convolucionais são geralmente armazenados em um tensor com dimensões N x H x W x C em que:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- N é o número de amostras de dados (tamanho do *minibatch*)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- H é a altura (*height*) do mapa de característica</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- W é a largura (*width*) do mapa de característica</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- C é o número de canais do mapa de característica</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Essa é a forma correta de se representar dados quando se realiza algo como uma convolução 2D, que necessita de informação sobre a distribuição espacial das informações. Porém, quando se utiliza camadas afim completamente conectadas para se processar uma imagem, deseja-se que cada amostra (imagem) seja representada como um único vetor -- não é útil se separar os diferentes canais, linhas e colunas dos dados. Assim, utiliza-se a operação de achatamento (`flatten`) para colapsar os valores `H x W x C` em um único vetor.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Vale observar que a chamada para `tf.reshape` possui como alvo o formato `(N, -1)` significando que a primeira dimensão será mantida e a demais colapsadas em uma única segunda dimensão.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**NOTA**: TensorFlow e PyTorch diferem sobre o layout default de um tensor; TensorFlow usa N x H x W x C porém PyTorch usa N x C x H x W.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;C8cZ7zemzQ-3&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def flatten(x):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;    \n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Input:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - TensorFlow Tensor of shape (N, D1, ..., DM)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Output:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - TensorFlow Tensor of shape (N, D1 * ... * DM)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    N = tf.shape(x)[0]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.reshape(x, (N, -1))&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore-input&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;KIAmUZJNzQ-6&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def test_flatten():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Construct concrete values of the input data x using numpy</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    x_np = np.arange(24).reshape((2, 3, 4))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print('x_np:</span><span class="s3">\\</span><span class="s1">n', x_np, '</span><span class="s3">\\</span><span class="s1">n')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Compute a concrete output value.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    x_flat_np = flatten(x_np)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print('x_flat_np:</span><span class="s3">\\</span><span class="s1">n', x_flat_np, '</span><span class="s3">\\</span><span class="s1">n')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;test_flatten()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;FcXDLMexzQ-8&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Definindo uma Rede de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, iremos implementar a primeira rede neural com TensorFlow: uma rede complementamente conectada com duas camadas usando ReLU e sem nenhum viés (termo independente) sobre o dataset CIFAR10. Por enquanto, utilizaremos apenas operações de baixo nível para definir a rede; mais adiante serão utilizadas abstrações mais elaboradas fornecidas pelo `tf.keras` para simplificar o processo.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;O *forward pass* da rede será definido na função `two_layer_fc` que receberá tensores com as entradas e pesos da rede e retorna um tensor com os valores de *score*. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Depois de definida a arquitetura de rede na função `two_layer_fc`, a implementação será testada checando-se as dimensões da saída.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**É muito importante que você leia e compreenda a implemtação abaixo.**&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;kJkBzE2OzQ-8&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def two_layer_fc(x, params):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    A fully-connected neural network; the architecture is:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    fully-connected layer -&gt; ReLU -&gt; fully connected layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Note that we only need to define the forward pass here; TensorFlow will take</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    care of computing the gradients for us.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    The input to the network will be a minibatch of data, of shape</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    and the output layer will produce scores for C classes.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      input data.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      network, where w1 has shape (D, H) and w2 has shape (H, C).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Returns:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      for the input data x.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    w1, w2 = params                   # Unpack the parameters</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    x = flatten(x)                    # Flatten the input; now x has shape (N, D)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return scores&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore-input&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;majrh8adzQ-_&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def two_layer_fc_test():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    hidden_layer_size = 42</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Scoping our TF operations under a tf.device context manager </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # lets us tell TensorFlow where we want these Tensors to be</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # multiplied and/or operated on, e.g. on a CPU or a GPU.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = tf.zeros((64, 32, 32, 3))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        w2 = tf.zeros((hidden_layer_size, 10))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Call our two_layer_fc function for the forward pass of the network.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = two_layer_fc(x, [w1, w2])</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print(scores.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;two_layer_fc_test()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;iXTMai2ZzQ_B&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Definindo uma Rede Convolucional (ConvNet) de Três Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora você deve completar a implementação da função `three_layer_convnet` que realiza o *forward pass* de uma rede convolucional de três camadas. A rede deve possuir a seguinte arquitetura:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Uma camada convolucional (com viés) com `channel_1` filtros, cada um deles com formato `KW1 x KH1`, e preenchimento (*zero-padding*) de tamanho 2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Uma camada convolucional (com viés) com `channel_2` filtros, cada um deles com formato `KW2 x KH2`, e preenchimento (*zero-padding*) de tamanho 1</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;4. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;5. Uma camada completamente conectada com viés, produzindo *scores* para `C` classes.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**DICA 1**: Para convoluções veja: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d; tenha cuidado com o preenchimento (*padding*)!</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**DICA 2**: Para viés veja: https://www.tensorflow.org/performance/xla/broadcasting&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;8Fx86L1jzQ_B&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def three_layer_convnet(x, params):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    A three-layer convolutional network with the architecture described above.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - params: A list of TensorFlow Tensors giving the weights and biases for the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      network; should contain the following:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        weights for the first convolutional layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        first convolutional layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        giving weights for the second convolutional layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        second convolutional layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        Can you figure out what the shape should be?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        Can you figure out what the shape should be?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    scores = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Implement the forward pass for the three-layer ConvNet.            #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                              END OF YOUR CODE                            #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return scores&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;kqerigvRzQ_F&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;Após definir o *forward pass* para a ConvNet de três camadas acima, execute o código da próxima célula para testar sua implementação. Semelhante a rede de duas camadas, o grafo será executado sobre tensores contendo zeros, portanto você deve assegurar que a função não tenham problemas de execução e que produz uma saída no formato correto.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Quando você executar essa função, `scores_np` deve ter dimensões `(64, 10)`.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;barebones_output_shape&quot;</span><span class="s0">,</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore-input&quot;</span><span class="s0">]},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def three_layer_convnet_test():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = tf.zeros((64, 32, 32, 3))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        conv_w1 = tf.zeros((5, 5, 3, 6))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        conv_b1 = tf.zeros((6,))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        conv_w2 = tf.zeros((3, 3, 6, 9))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        conv_b2 = tf.zeros((9,))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        fc_w = tf.zeros((32 * 32 * 9, 10))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        fc_b = tf.zeros((10,))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = three_layer_convnet(x, params)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Inputs to convolutional layers are 4-dimensional arrays with shape</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # [batch_size, height, width, channels]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print('scores_np has shape: ', scores.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;three_layer_convnet_test()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;1Al2zJsfzQ_I&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Passo de Treinamento</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora definiremos a função `training_step` para realizar um único passo de treinamento. Isso exigirá 03 etapas básicas:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Computar a perda</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Computar o gradiente da perda em relação a todos os pesos da rede</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Fazer um passo de atulização de pesos utilizando SGD (*stochastic gradient descent*).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;São necessárias algumas poucas funções do TensorFlow para se realizar isso:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Para cálculo da perda por entropia cruzada, pode-se utilizar `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Para se obter a média da perda para as amostras de dados do *minibatch*, pode-se usar `tf.reduce_mean`:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reduce_mean</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Para computar o gradiente da perda em relação aos pesos, pode-se usar `tf.GradientTape` (útil para execução rápida):  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Já a modificação doa valores de pesos armazenados em um tensor, pode-se usar `tf.assign_sub` (</span><span class="s3">\&quot;</span><span class="s1">sub</span><span class="s3">\&quot; </span><span class="s1">é para subtração): https://www.tensorflow.org/api_docs/python/tf/assign_sub </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;gj4eFOCfzQ_I&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def training_step(model_fn, x, y, params, learning_rate):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.GradientTape() as tape:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = model_fn(x, params) # Forward pass of the model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        total_loss = tf.reduce_mean(loss)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        grad_params = tape.gradient(total_loss, params)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Make a vanilla gradient descent step on all of the model parameters</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Manually update the weights using assign_sub()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        for w, grad_w in zip(params, grad_params):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            w.assign_sub(learning_rate * grad_w)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        return total_loss&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;U-gJW49uzQ_K&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def train_part2(model_fn, init_fn, learning_rate):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Train a model on CIFAR-10.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - model_fn: A Python function that performs the forward pass of the model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      using TensorFlow; it should have the following signature:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      minibatch of image data, params is a list of TensorFlow Tensors holding</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      the model weights, and scores is a TensorFlow Tensor of shape (N, C)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      giving scores for all elements of x.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - init_fn: A Python function that initializes the parameters of the model.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      It should have the signature params = init_fn() where params is a list</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      of TensorFlow Tensors holding the (randomly initialized) weights of the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      model.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - learning_rate: Python float giving the learning rate to use for SGD.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    params = init_fn()  # Initialize the model parameters            </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    for t, (x_np, y_np) in enumerate(train_dset):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Run the graph on a batch of training data.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        loss = training_step(model_fn, x_np, y_np, params, learning_rate)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Periodically print the loss and check accuracy on the val set.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        if t % print_every == 0:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            print('Iteration %d, loss = %.4f' % (t, loss))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            check_accuracy(val_dset, x_np, model_fn, params)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;F-s7lPbJzQ_N&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def check_accuracy(dset, x, model_fn, params):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Check accuracy on a classification model, e.g. for validation.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - dset: A Dataset object against which to check accuracy</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - x: A TensorFlow placeholder Tensor where input images should be fed</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - model_fn: the Model we will be calling to make predictions on x</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - params: parameters for the model_fn to work with</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Returns: Nothing, but prints the accuracy of the model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    num_correct, num_samples = 0, 0</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    for x_batch, y_batch in dset:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores_np = model_fn(x_batch, params).numpy()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        y_pred = scores_np.argmax(axis=1)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        num_samples += x_batch.shape[0]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        num_correct += (y_pred == y_batch).sum()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    acc = float(num_correct) / num_samples</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;AiYGgKPRzQ_R&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Inicialização</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Pode-se utilizar a seguinte função para se inicializar as matrizes de pesos utilizando o método de normalização de Kaiming (He et al. 2015)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;[1] He et al., *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;qr7k0N8qzQ_R&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def create_matrix_with_kaiming_normal(shape):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    if len(shape) == 2:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        fan_in, fan_out = shape[0], shape[1]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    elif len(shape) == 4:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        fan_in, fan_out = np.prod(shape[:3]), shape[3]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;hMqE8Wr9zQ_V&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Treinando uma Rede de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, estamos prontos para utilizar todas as funções definidas acima no treinamento de uma rede de duas camadas completamente conectada sobre o dataset CIFAR-10.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Basta então definir uma função para inicializar os pesos do modelo e chamar, em seguida, a função `train_part2`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Para se definir os pesos da rede é necessário se introduzir uma outra peça importante da API do TensorFLow: `tf.Variable`. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Uma variável do TensorFlow é um tensor cujo valor é armazenado em um grafo e que persiste entre várias execuções do grafo de computação; entretanto diferente de constantes definidas com `tf.zeros` ou `tf.random_normal`, os valores de uma variável podem mudar na medida que o grafo executa; essas alterações irão persistir entre diferentes execuções do gafo. Assim, parâmetros da rede que são aprendidos são geralmente armazenados em variáveis.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 40% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;YiUdiMNTzQ_W&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def two_layer_fc_init():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Initialize the weights of a two-layer network, for use with the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    two_layer_network function defined above. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    You can use the `create_matrix_with_kaiming_normal` helper!</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs: None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Returns: A list of:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - w1: TensorFlow tf.Variable giving the weights for the first layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - w2: TensorFlow tf.Variable giving the weights for the second layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    hidden_layer_size = 4000</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return [w1, w2]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;learning_rate = 1e-2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part2(two_layer_fc, two_layer_fc_init, learning_rate)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;C1ly58u0zQ_Y&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Fundamentos do TensorFlow: Treinando uma ConvNet de Três Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, você deve usar o TensorFlow para treinar uma ConvNet de 3 camadas sobre o dataset CIFAR-10.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você deve implementar a função `three_layer_convnet_init`. Lembre-se de que a arquitetura da rede é a seguinte:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Camada convolucional (com viés) com 32 filtros 5x5, e com preenchimento (*zero-padding*) de tamanho 2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Camada convolucional (com viés) com 16 filtros 3x3, e com preenchimento (*zero-padding*) de tamanho 1</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;4. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;5. Camada completamente conectada (com viés) para computar *scores* para 10 classes</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 43% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;barebones_accuracy&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def three_layer_convnet_init():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Initialize the weights of a Three-Layer ConvNet, for use with the</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    three_layer_convnet function defined above.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    You can use the `create_matrix_with_kaiming_normal` helper!</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs: None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Returns a list containing:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - conv_w1: TensorFlow tf.Variable giving weights for the first conv layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - conv_b1: TensorFlow tf.Variable giving biases for the first conv layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - conv_w2: TensorFlow tf.Variable giving weights for the second conv layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - conv_b2: TensorFlow tf.Variable giving biases for the second conv layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - fc_w: TensorFlow tf.Variable giving weights for the fully-connected layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - fc_b: TensorFlow tf.Variable giving biases for the fully-connected layer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    params = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Initialize the parameters of the three-layer network.              #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                             END OF YOUR CODE                             #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return params</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;learning_rate = 3e-3</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;w1IxkvmNzQ_a&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# API Keras Model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Implementar uma rede neural utilizando a API de baixo nível do TensorFlow é uma boa forma de entender como o TensorFlow funciona, porém é um pouco inconveniente, pois se deve manualmente manter e acompanhar todos os tensores armazenando parâmetros que serão aprendidos. Isso é simples para uma rede pequena, mas pode rapidamente se tornar impraticável para modelos grandes e complexos.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Felizmente, o TensorFlow 2.0 fornece APIs de alto nível como `tf.keras` que facilitam a construção de modelos a partir de camadas modulares. Além disso, o TensorFlow 2.0 utilizar a execução rápida que avalia operações de forma imediata sem a necessidade de construção explíta de nenhum grafo de computação. Isso torna mais fácil de se escrever e depurar modelos.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nessa parte desse *notebook*, você irá definir um modelo de rede neural usando a API `tf.keras.Model`. Para implementar seu próprio modelo, você precisará do seguinte:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Definir uma nova classe que seja herdeira de `tf.keras.Model`. Você deve dar um nome intuitivo a sua classe que sirva para descrevê-la, como `TwoLayerFC` ou `ThreeLayerConvNet`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. No método de inicialização `__init__()` para sua nova classe, você deve definir todas as camadas como atributos de classe. O pacote `tf.keras.layers` fornece muitas camadas comuns as redes neurais, como `tf.keras.layers.Dense` para camadas completamente conectadas e `tf.keras.layers.Conv2D` para camadas convolucionais. Internamente, essas camadas irão construir variáveis (tensores) para quaisquer parâmetros que devam ser aprendidos. **AVISO: Não se esqueça de chamar `super(YourModelName, self).__init__()` como a primeira instrução de seu inicializador!**</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Implementar o método `call()` para sua classe, que será responsável pelo cálculo do *forward pass* de seu modelo, e estabelece a *conectividade* entre os elementos de sua rede. Camadas definidas em no método `__init__()` são utilizadas para implementar o método `__call__()`. Assim, as camadas podem seu usadas para transformar tensores de entrada em tensores de saída. Não define nenhuma nova camada no método `call()`; qualquer camada que você deseje utilizar no *forward pass* deve ser definida no método `__init__()`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Após definir sua subclasse de `tf.keras.Model`, você pode criar uma instância dela utilizá-la como um modelo.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Exemplo de Subclasse de Keras Model: Rede de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Eis agora um exemplo concreto de uso da API `tf.keras.Model` para se definir uma rede de duas camadas. Existem alguns pequenos pontos da API a se prestar atenção aqui:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Será utilizado um objeto `Initializer` para definir valores iniciais para os parâmetros/pesos (a serem aprendidos) nas camadas; em particular `tf.initializers.VarianceScaling` fornece um comportamento similar ao método de inicialização de Kaiming utilizado anteriormente nesse *notebook* (ver He et al. 2015). Você pode encontrar mais detalhes em https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/initializers/VarianceScaling</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Serão utilizado objetos `tf.keras.layers.Dense`para representar as duas camadas completamente conectadas do modelo. Além de multiplicar suas entradas por uma matriz de pesos e adicionar um vetor de viés, essas camadas também podem aplicar uma não-linearidade para você. Para a primeira camada, especifica-se um função de ativação ReLU passando o parâmetro `activation='relu'` para o construtor; já a segunda camada usa uma função de ativação *softmax*. Por fim, utiliza-se `tf.keras.layers.Flatten` para achatar o tensor de entrada antes das camadas completamente conectadas.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore-input&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;wt7PWXtjzQ_b&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;class TwoLayerFC(tf.keras.Model):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def __init__(self, hidden_size, num_classes):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        super(TwoLayerFC, self).__init__()        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        initializer = tf.initializers.VarianceScaling(scale=2.0)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                   kernel_initializer=initializer)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                   kernel_initializer=initializer)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        self.flatten = tf.keras.layers.Flatten()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def call(self, x, training=False):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = self.flatten(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = self.fc1(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = self.fc2(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        return x</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def test_TwoLayerFC():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot; </span><span class="s1">A small unit test to exercise the TwoLayerFC model above. </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    input_size, hidden_size, num_classes = 50, 42, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    x = tf.zeros((64, input_size))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = TwoLayerFC(hidden_size, num_classes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = model(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        print(scores.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;test_TwoLayerFC()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;aUZzlSbvzQ_d&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Model: Definindo uma ConvNet de Três Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, você deve implementar uma ConvNet de 3 camadas usando a API `tf.keras.Model`. Seu modelo deve a mesma arquitetura utilizada anteriormente neste *notebook*:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Camada convolucional com filtros 5x5 e com preenchimento (*zero-padding*) de tamanho 2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Camada convolucional com filtros 3x3 e com preenchimento (*zero-padding*) de tamanho 1</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;4. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;5. Camada completamente conectada (com viés) para computar *scores* para classes</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;6. Não-linearidade Softmax</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você deve inicializar os pesos de sua rede utilizando o mesmo método usado na rede de duas camada acima.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**DICA**: Para conseguir implementar, explore a documentação de `tf.keras.layers.Conv2D` e `tf.keras.layers.Dense`:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;S9cBMCvazQ_d&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;class ThreeLayerConvNet(tf.keras.Model):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def __init__(self, channel_1, channel_2, num_classes):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        super(ThreeLayerConvNet, self).__init__()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # should instantiate layer objects to be used in the forward pass.     #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        #                           END OF YOUR CODE                           #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def call(self, x, training=False):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # TODO: Implement the forward pass for a three-layer ConvNet. You      #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # should use the layer objects defined in the __init__ method.         #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        #                           END OF YOUR CODE                           #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ########################################################################        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        return scores&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;INM14tQSzQ_f&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;Uma vez que você termine a implementaçaõ da classe `ThreeLayerConvNet` acima, você pode executar o seguinte código para assegurar que sua implementação não contém de execução e que produz saída no formato esperado.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;keras_model_output_shape&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def test_ThreeLayerConvNet():    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    channel_1, channel_2, num_classes = 12, 8, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        x = tf.zeros((64, 3, 32, 32))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = model(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        print(scores.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;test_ThreeLayerConvNet()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;hS1EaZ8BzQ_h&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Model: Treinamento Rápido</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Apesar de moelos em Keras possuirem um método de treinamento embutido (por meio de `model.fit`), as vezes é necessário se *customizar* o treinamento. A seguir, vocÊ verá um exemplo de um método de treinamento implementado por meio de execução rápida.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Em particular, observe o uso de `tf.GradientTape`. Diferenciação automática é utilizada para implementar a propagação retrógrada em `frameworks` como o TensorFlow. Durante a execução rápida, `tf.GradientTape` é usado para cálculo de gadientes. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Além disso, TensorFlow 2.0 possui um mecanismo simplificadopara avaliação de métricas por meio do módulo `tf.keras.metrics`. Cada métrica é um objeto. Pode-se adicionar observações por meio de `update_state()` e eliminar todas elas usando `reset_state()`. Para se obter o valor corrente de uma métrica, usa-se o método `result()` sobre o respectivo objeto. &quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;jmmkBcBczQ_i&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Simple training loop for use with models defined using tf.keras. It trains</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    a model for one epoch on the CIFAR-10 training set and periodically checks</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    accuracy on the CIFAR-10 validation set.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Inputs:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - model_init_fn: A function that takes no parameters; when called it</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      constructs the model we want to train: model = model_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - optimizer_init_fn: A function which takes no parameters; when called it</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      constructs the Optimizer object we will use to optimize the model:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      optimizer = optimizer_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    - num_epochs: The number of epochs to train for</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    Returns: Nothing, but prints progress during trainingn</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot;    \n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # Compute the loss like we did in Part II</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        model = model_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        optimizer = optimizer_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        train_loss = tf.keras.metrics.Mean(name='train_loss')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        val_loss = tf.keras.metrics.Mean(name='val_loss')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        t = 0</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        for epoch in range(num_epochs):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            train_loss.reset_states()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            train_accuracy.reset_states()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;            for x_np, y_np in train_dset:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                with tf.GradientTape() as tape:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    # Use the model function to build the forward pass.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    scores = model(x_np, training=is_training)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    loss = loss_fn(y_np, scores)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;      </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    gradients = tape.gradient(loss, model.trainable_variables)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    # Update the metrics</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    train_loss.update_state(loss)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    train_accuracy.update_state(y_np, scores)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    if t % print_every == 0:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        val_loss.reset_states()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        val_accuracy.reset_states()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        for test_x, test_y in val_dset:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                            # During validation at end of epoch, training set to False</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                            prediction = model(test_x, training=False)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                            t_loss = loss_fn(test_y, prediction)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                            val_loss.update_state(t_loss)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                            val_accuracy.update_state(test_y, prediction)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                        print (template.format(t, epoch+1,</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                             train_loss.result(),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                             train_accuracy.result()*100,</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                             val_loss.result(),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                             val_accuracy.result()*100))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                    t += 1&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;O42Gyk12zQ_k&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Model: Treinando uma Rede de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, pode-se utilizar as ferramentar definidas anteriormente para se treinar uma rede de duas camadas sobre o dataset CIFAR-10. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Definiu-se os métodos `model_init_fn` e `optimizer_init_fn` que controem o modelo e executam a otimização do mesmo, respectivamente. </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Deseja-se treinar o modelo por meio do SGD (*stochastic gradient descent*) sem uso de momento, portanto, utiliza-se a função `tf.keras.optimizers.SGD`; você pode [ler mais a esse respeito aqui](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD).</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 40% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;RDsrltTIzQ_k&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;hidden_size, num_classes = 4000, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;learning_rate = 1e-2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return TwoLayerFC(hidden_size, num_classes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.keras.optimizers.SGD(learning_rate=learning_rate)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;rLcWSbkwzQ_n&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Model: Treinando uma ConvNet de Três Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora você deve utilizar as ferramentas definidas anteriormente para treinar uma ConvNet de 3 camadas sobre o dataset CIFAR-10. Sua ConvNet deve possuir 32 filtros na primeira camada convolucional e 16 filtros na segunda camada.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Para treinar o modelo, você deve adotar o método do gradiente com o momento de Nesterov de 0,9.  </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;**DICA**: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 50% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;keras_model_accuracy&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;learning_rate = 3e-3</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;channel_1, channel_2, num_classes = 32, 16, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Complete the implementation of model_fn.                           #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                           END OF YOUR CODE                               #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    optimizer = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Complete the implementation of model_fn.                           #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                           END OF YOUR CODE                               #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return optimizer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;9EHF2BUKzQ_p&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# API Keras Sequential</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;As implementações com a API `tf.keras.Model` permitem que você defina  modelos com qualquer número de camadas e com uma conectividade arbitrária entre elas.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Contudo, para muitos modelos você não necessita de tamanha flexibilidade - grande parte dos modelos pode ser expressa como uma pilha sequencial de camadas em que a saída de uma camada é usada como entrada da próxima.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Caso seu modelo se encaixe nesse padrão, então existe um forma ainda mais fácil de se definir o modelo por meio do uso da API `tf.keras.Sequential`. Você não precisa escrever nenhuma classe customizada; basta chamar o construtor `tf.keras.Sequential` com uma lista contendo a senquência de camadas.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Uma dificuldade no uso de `tf.keras.Sequential` é que você deve definir as dimensões da entrada do modelo e informar isso (por meio do valor de `input_shape`) à primeira camada de seu modelo.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Uso da API Keras Sequential: Rede de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora, a rede completamente conectada de duas camadas será reescrita usando `tf.keras.Sequential`, e treinada com o método definido acima.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 40% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;s1i3FzxfzQ_q&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;learning_rate = 1e-2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    input_shape = (32, 32, 3)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    hidden_layer_size, num_classes = 4000, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    initializer = tf.initializers.VarianceScaling(scale=2.0)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    layers = [</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        tf.keras.layers.Flatten(input_shape=input_shape),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        tf.keras.layers.Dense(hidden_layer_size, activation='relu',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                              kernel_initializer=initializer),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        tf.keras.layers.Dense(num_classes, activation='softmax', </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                              kernel_initializer=initializer),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ]</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = tf.keras.Sequential(layers)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.keras.optimizers.SGD(learning_rate=learning_rate) </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;ZtmDy6HCzQ_s&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Abstraindo o Método de Treinamento</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nos exemplos anteriores, foi utilizado um método de treinamento customizado para se treinar os modelos (p.ex. `train_part34`). Excrever seu próprio método de treinamento só é necessário se você desejar mais flexibilidade e controle durante o treinamento de seu modelo. Alternativamente, você pode utilizar de métodos prontos da API como `tf.keras.Model.fit()` e `tf.keras.Model.evaluate` para treinar e avaliar um modelo. Nesse caso, você deve lembrar de configurar antes seu modelo para treinamento chamando `tf.keras.Model.compile`.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 42% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;Wak9IfnjzQ_s&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;model = model_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;              loss='sparse_categorical_crossentropy',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;              metrics=[tf.keras.metrics.sparse_categorical_accuracy])</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.evaluate(X_test, y_test)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;QG8cXNHszQ_u&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Sequential: ConvNet de Três Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora você deve usar `tf.keras.Sequential` para reimplementar a  arquitetura da ConvNet de 3 camadas anterior. Como lembrete, seu modelo deve possuir a seguinte arquitetura:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;1. Camada convolucional com filtros 5x5 e com preenchimento (*zero-padding*) de tamanho 2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;2. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;3. Camada convolucional com filtros 3x3 e com preenchimento (*zero-padding*) de tamanho 1</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;4. Não-linearidade ReLU</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;5. Camada completamente conectada (com viés) para computar *scores* para classes</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;6. Não-linearidade Softmax</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você deve inicializar os pesos do modelo usando `tf.initializers.VarianceScaling` como antes e o modelo deve ser treinado com </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;momento de Nesterov de 0,9.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 45% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;keras_sequential_accuracy&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                            END OF YOUR CODE                              #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;learning_rate = 5e-4</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    optimizer = None</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # TODO: Complete the implementation of model_fn.                           #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    #                           END OF YOUR CODE                               #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return optimizer</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;hmkfljZuzQ_x&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;Você também pode treinar esse modelo utilizando o método pronto da API do TensorFlow.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;zWlS3EzazQ_y&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;model = model_init_fn()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.compile(optimizer='sgd',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;              loss='sparse_categorical_crossentropy',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;              metrics=[tf.keras.metrics.sparse_categorical_accuracy])</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model.evaluate(X_test, y_test)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;451wYP6xzQ_z&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;##  API Funcional</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Demonstração com uma Rede de Duas Camadas </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nas seções anteriores desse *notebook*, vimos como usar `tf.keras.Sequential` para empilhar camadas de forma a se construir rapidamente modelos simples. Porém isto possui um custo: a perda de flexibilidade.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Em alguns casos, deseja-se escrever modelos complexos que possuem um fluxo de dados não-sequencial: uma camada pode ter **múltiplas entradas e/ou saídas**, como, por exemplo, concatenar as saídas de duas camadas anteriores para se fornecer como entrada para uma próxima! Alguns exemplos disso são conexões residuais e os blocos densos da ResNet e da DenseNet, respectivamente.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nesses casos, pode-se utilizar a API funcional do Keras para se escrever modelos com topologias complexas como:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot; 1. Modelos com múltiplas entradas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot; 2. Modelos com múltiplas saídas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot; 3. Modelos com camadas compartilhadas (a mesma camada chamada várias vezes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot; 4. Modelos com fluxos de dados não-sequenciais (p.ex. conexões residuais)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Para a escrita de um modelo com a API funcional, você deve criar uma instância de `tf.keras.Model` e explicitamente fornecer os tensores de entrada e de saída para seu modelo. &quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;tags&quot;</span><span class="s0">:[</span><span class="s1">&quot;pdf-ignore&quot;</span><span class="s0">],</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;GU8Vr1uCzQ_0&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;def two_layer_fc_functional(input_shape, hidden_size, num_classes):  </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    initializer = tf.initializers.VarianceScaling(scale=2.0)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    inputs = tf.keras.Input(shape=input_shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    flattened_inputs = tf.keras.layers.Flatten()(inputs)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                                 kernel_initializer=initializer)(flattened_inputs)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    scores = tf.keras.layers.Dense(num_classes, activation='softmax',</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;                             kernel_initializer=initializer)(fc1_output)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    # Instantiate the model given inputs and outputs.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = tf.keras.Model(inputs=inputs, outputs=scores)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return model</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def test_two_layer_fc_functional():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\&quot;\&quot;\&quot; </span><span class="s1">A small unit test to exercise the TwoLayerFC model above. </span><span class="s3">\&quot;\&quot;\&quot;\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    input_size, hidden_size, num_classes = 50, 42, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    input_shape = (50,)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    x = tf.zeros((64, input_size))</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    with tf.device(device):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        scores = model(x)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        print(scores.shape)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;test_two_layer_fc_functional()&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;UGiKqnqpzQ_2&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;### Uso da API Keras Funcional: Treinando uma de Duas Camadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Agora você pode treinar a rede de duas camadas construída (ver acima) utilizando a API funcional.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você não precisa ajustar nenhum hiperparâmetro, mas (mesmo assim) deve alcançar uma acurácia de validação acima de 40% após uma época de treinamento.&quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;vN4YGTlkzQ_2&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;input_shape = (32, 32, 3)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;hidden_size, num_classes = 4000, 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;learning_rate = 1e-2</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return two_layer_fc_functional(input_shape, hidden_size, num_classes)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.keras.optimizers.SGD(learning_rate=learning_rate)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;markdown&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;SyLho7rizQ_5&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;# Desafio Final: Treinar uma ConvNet sobre o Dataset CIFAR-10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Nesta seção você poderá experimentar com uma arquitetura de ConvNet que desejar treinar sobre o dataset CIFAR-10.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você deve experimentar diferentes arquiteturas, hoperparâmetros, funções de perda, regularização ou quaisquer outros recursos que deseje para treinar um modelo que alcance **pelo menos 70%** de acurácia sobre o conjunto de **validação** dentro de 10 épocas.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Você pode utilizar as funções de treinamento prontas da API, a função `train_part34` dada acima ou mesmo implementar sua própria função.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Ao final, é importante que você consiga descrever o que você fez para alcançar seu resultado!</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Algumas ideias que você pode experimentar:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Tamanho de filtros**: Acima foram usados filtros 5x5 e 3x3; esses tamanhos são ideiais?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Número de filtros**: Acima foram usados 16 e 32 filtros. Será que um aumento ou redução dessas quantidades produziria um resultado melhor?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Agrupamento (*Pooling*)**: Acima não se utilizou de nenhuma camada de agrupamento (*pooling*). Isso poderia melhorar o modelo?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Normalização**: Será que o modelo seria melhor se fosse adotada alguma técnica de normalização, como, por exemplo, *batch normalization*?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Arquitetura da Rede**: A ConvNet anterior possui apenas 3 camadas. Um modelo mais profundo teria resultados melhores?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Agrupamento Médio Global (*Global average pooling*)**: Se, ao invés de achatamento (*flattening*) depois da última camada convolucional, fosse utilizado um agrupamento médio global (*global average pooling*) o resultado seria melhor? Essa estratágia é usada por exemplo na GoogLeNet e na ResNet.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- **Regularização**: Será que o uso de alguma forma de regularização (como, decaimento de taxa e *dropout*, por exemplo) melhoria o resultado do modelo?</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### NOTA: *Batch Normalization / Dropout*</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Se você for utilizar *batch normalization* e *drpout*, lembre-se de fornecer o parâmetro `is_training=True` caso utilize o método de treinamento  `train_part34()`. Camadas **BatchNorm** e **Dropout** possuem comportamentos diferentes durante o treinamento e seu posterior uso na inferência.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;`training` é um argumento específico reservado para esse propósito em qualquer função `call()` da API  `tf.keras.Model`. Mais informações sobre isso em https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Dicas para Treinamento</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Para cada arquitetura de rede que você experimentar, você deve ajustar a taxa de aprendizado e demais hiperparâmetros. Quando realizar isso, algumas aspectos devem ser considerados:</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Se os hiperparâmetros são bons, você deve observar melhorias dentro de algumas centenas de iterações</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Lembre-se de usar uma abordagem de refinamento sucessivos (*coarse-to-fine*) nos ajustes de hiperparâmetros: inicie com intervalos grandes e faça poucas iterações de forma a determinar combinações promissoras para serem exploradas mais cuidadosamente a seguir </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Uma vez que tenha encontrado alguns conjuntos de parâmetros que parecem funcionar, faça uma busca mais refinada em torno desses valores de parâmetros. Talvez seja necessário treinar por um número maior de épocas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Você deve usar o conjunto de validação para ajuste de hiperparâmetros (**nunca o conjunto de teste!**), e deixar reservado o conjunto de teste para avaliar sua arquitetura final obtida com os melhores parâmetros selecionados por meio do conjunto de validação</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Indo adiante e além...</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;Caso você deseje existem inúmeros recursos e estratégias adicionais que você pode experimentar para melhorar a performance de seu modelo. Você **não precisa** implementar nenhuma delas, mas não deixe a diversão de lado, caso tenha tempo disponível!</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Otimizadores alternativos: você pode experimentar usar Adam, Adagrad, RMSprop, etc.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Funções de ativação alternativas como *Leaky ReLU*, *Parametric ReLU*, ELU, ou MaxOut.</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Construir conjuntos de modelos (*model ensembles*)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Realizar *data augmentation*</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;- Explorar novas arquiteturas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;  - [ResNets](https://arxiv.org/abs/1512.03385) em que a entrada da camada anterior é adicionada a saída</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;  - [DenseNets](https://arxiv.org/abs/1608.06993) em que entradas de camadas anteriores são concatenadas</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;  - [Eis um *blog* interessante com mais ideias](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;  </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;### Tenha uma boa (e divertida) experiência de treinamento! &quot;</span><span class="s0">]},{</span><span class="s1">&quot;cell_type&quot;</span><span class="s0">:</span><span class="s1">&quot;code&quot;</span><span class="s0">,</span><span class="s1">&quot;metadata&quot;</span><span class="s0">:{</span><span class="s1">&quot;id&quot;</span><span class="s0">:</span><span class="s1">&quot;open_ended_accuracy&quot;</span><span class="s0">},</span><span class="s1">&quot;source&quot;</span><span class="s0">:[</span><span class="s1">&quot;class CustomConvNet(tf.keras.Model):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def __init__(self):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        super(CustomConvNet, self).__init__()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # TODO: Construct a model that performs well on CIFAR-10                   #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        #                            END OF YOUR CODE                              #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    def call(self, input_tensor, training=False):</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # TODO: Construct a model that performs well on CIFAR-10                   #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        pass</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        #                            END OF YOUR CODE                              #</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        ############################################################################</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;        return x</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;print_every = 700</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;num_epochs = 10</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;model = CustomConvNet()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def model_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return CustomConvNet()</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;def optimizer_init_fn():</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    learning_rate = 1e-3</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;    return tf.keras.optimizers.Adam(learning_rate) </span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;</span><span class="s3">\n</span><span class="s1">&quot;</span><span class="s0">,</span><span class="s1">&quot;train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)&quot;</span><span class="s0">],</span><span class="s1">&quot;execution_count&quot;</span><span class="s0">:</span><span class="s3">null</span><span class="s0">,</span><span class="s1">&quot;outputs&quot;</span><span class="s0">:[]}]}</span></pre>
</body>
</html>