{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento\n",
        "\n",
        "Notebook para pré-processamento de dados textuais\n",
        "\n",
        "## Carregando pacotes iniciais"
      ],
      "metadata": {
        "id": "HDKmT_TZsC0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode==1.2.0\n",
        "!pip install wikipedia==1.4.0\n",
        "!pip install spacy==2.2.4"
      ],
      "metadata": {
        "id": "r1MYWfZ51GSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4sgxYq-5PzQ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import wikipedia\n",
        "import re\n",
        "import spacy\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando dados da wikipidia"
      ],
      "metadata": {
        "id": "PaT1-s9psR31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia.set_lang(\"pt\")"
      ],
      "metadata": {
        "id": "8dXQGNCj1B6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pln = wikipedia.page(\"PLN\")"
      ],
      "metadata": {
        "id": "jku45cYn1Dal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pln.conte"
      ],
      "metadata": {
        "id": "4boqf6UmnnmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pln.content"
      ],
      "metadata": {
        "id": "-httKFRa1OU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "id": "WMMhVr-01Qar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"O texto que estamos utilizando é da URL\",pln.url)"
      ],
      "metadata": {
        "id": "QlWfgbR21QU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenização\n",
        "Primeiro pré-processamento é de tokenização. Consideramos tokens o menor nível de representação do texto; por exemplo palavras.\n",
        "\n",
        "Abaixo são apresentado três metodologias para tokenização.\n",
        "\n",
        "1. Utilizando a função split, usualmente tokenizamos usando o espaço para quebrar os tokens.\n",
        "2. Utilizando regex\n",
        "Na regex usada de exemplo '\\w+(?:'\\w+)?|[^\\w\\s]' onde temos um grupo de caracter alfanumerico, podendo ou não ter apostofro, seguido de alfanumerico ou um grupo com caracter que não é alfanumerico e nem espaçamento.\n",
        "3. Utilizando a função  word_tokenize do nltk"
      ],
      "metadata": {
        "id": "B5FRpU8n1YGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_split = corpus.split(\" \")"
      ],
      "metadata": {
        "id": "LcQ29oMM1Xlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_regex = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)"
      ],
      "metadata": {
        "id": "li5EX14i1c-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(corpus, language='portuguese')\n",
        "tokens_nltk = nltk.word_tokenize(corpus, language='portuguese')"
      ],
      "metadata": {
        "id": "JwaG64Z_2sla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_frequencia_tokens(tokens):\n",
        "    fd_words = FreqDist(tokens)\n",
        "    fd_words.plot(20)"
      ],
      "metadata": {
        "id": "I-CNKzWs2x0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_split)"
      ],
      "metadata": {
        "id": "Ng1EkkbVQhwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_regex)"
      ],
      "metadata": {
        "id": "v1bBX19MQYfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_nltk)"
      ],
      "metadata": {
        "id": "lAG5QSjRPAm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_nltk))"
      ],
      "metadata": {
        "id": "zjBOT0P3SNkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoção de stopWords\n",
        "\n",
        "Para remover stopword utilizaremos as palavras já definidas no pacote stopwords; para remover as stopwords iremos validar qual token não esta no conjunto de stopwords."
      ],
      "metadata": {
        "id": "0JfgvVM-2zaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "portugues_stops = stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "OTbrRgPE26BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(portugues_stops)"
      ],
      "metadata": {
        "id": "XPWLxt113PjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(portugues_stops)"
      ],
      "metadata": {
        "id": "ME6_5elrQuSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sem_stop = [t for t in tokens_regex if t not in portugues_stops]"
      ],
      "metadata": {
        "id": "bdrEcCeM3P-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_sem_stop)"
      ],
      "metadata": {
        "id": "9TMYuyZzRCm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_sem_stop))"
      ],
      "metadata": {
        "id": "H4WRhlTCSWEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capitalização\n",
        "Para esse processo iremos passar as letras para minusculo ou maiusculo, para isso usaremos as funções do python (.lower() e .upper())"
      ],
      "metadata": {
        "id": "RRdkg0oX3WCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [t.lower() for t in tokens_sem_stop]"
      ],
      "metadata": {
        "id": "caSy_YDT3YG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens)"
      ],
      "metadata": {
        "id": "OAJ9Yu3QRUMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens))"
      ],
      "metadata": {
        "id": "9lQ9J_OMSZ30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoção de números\n",
        "Para remover ou substituir números iremos usar a função sub de re; esse processamento pode ser feito antes ou depois da tokenização."
      ],
      "metadata": {
        "id": "oei-Yelb3cqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sem_numbers = [re.sub(r'\\d', '', t) for t in tokens]"
      ],
      "metadata": {
        "id": "1jeWa5UP3caE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_sem_numbers)"
      ],
      "metadata": {
        "id": "exzLclS7Rgpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_sem_numbers))"
      ],
      "metadata": {
        "id": "S29k7x8FShF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoção de pontos\n",
        "Para remoção iremos criar um conjunto de strings que representam pontos, para obter esse conjunto iremos usar o objeto punctuation do pacote string"
      ],
      "metadata": {
        "id": "3yU8vWyZ3oPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "pwCUCXKY3qVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sem_punction = [t for t in tokens_sem_numbers if t not in string.punctuation]"
      ],
      "metadata": {
        "id": "8NDfvUa13vm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_sem_punction)"
      ],
      "metadata": {
        "id": "ANTjq6GcRp8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_sem_punction))"
      ],
      "metadata": {
        "id": "FhleeS2JSkWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remoção de acentos\n",
        "Para a remoção de acentos iremos usar a função unidecode do pacote unidecode. Essa função será utilizada em cada token."
      ],
      "metadata": {
        "id": "WKNSRbDY3x-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "bKOsEvob31hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unidecode('é')"
      ],
      "metadata": {
        "id": "SN2MIXV7RyRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sem_acentos = [unidecode(t) for t in tokens_sem_punction]"
      ],
      "metadata": {
        "id": "FbQuw_YD32Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_sem_acentos)"
      ],
      "metadata": {
        "id": "YZq-99LtR2O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_sem_acentos))"
      ],
      "metadata": {
        "id": "ClYUYpihSnDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Para realizar stemming será usado o modelo do pacote nltk. Lembrando que será criado palavras que podem não existir."
      ],
      "metadata": {
        "id": "XgyudewD4N4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "metadata": {
        "id": "g4c3pfUs4Cvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sem_acentos[15]"
      ],
      "metadata": {
        "id": "SpjoarJ7R-XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('linguas')"
      ],
      "metadata": {
        "id": "fcXB6AuPSAf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_stemmer = [stemmer.stem(t) for t in tokens_sem_acentos]"
      ],
      "metadata": {
        "id": "xPlF37X34TZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(tokens_stemmer)"
      ],
      "metadata": {
        "id": "XroNTQ_pSFEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens_stemmer))"
      ],
      "metadata": {
        "id": "je3lu9jBSqHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Lemmanization\n",
        "\n",
        "Para realizar lemmanization temos que carregar o modelo a ser usada para isso usaremos os modelos disponiveis no pacote [spaCy](https://spacy.io/). Para o exemplo usaremos o modelo [pt_core_news_sm](https://spacy.io/models/pt#pt_core_news_sm)"
      ],
      "metadata": {
        "id": "PExuB_il4VMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pt_core_news_sm"
      ],
      "metadata": {
        "id": "zoeHXppU4W1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pt_core_news_sm.load()"
      ],
      "metadata": {
        "id": "cyN-rVOE4bSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_tokens = ' '.join(tokens_sem_acentos)\n",
        "str_tokens"
      ],
      "metadata": {
        "id": "toIYT-2R4kkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(str_tokens)"
      ],
      "metadata": {
        "id": "MKygbU_94rJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_lemm = [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "2YNAlxmh4tPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_frequencia_tokens(token_lemm)"
      ],
      "metadata": {
        "id": "DiNCRuIFTQs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(token_lemm))"
      ],
      "metadata": {
        "id": "6H9RBiNSTMWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}