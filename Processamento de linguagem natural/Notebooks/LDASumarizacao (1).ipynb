{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QiuQd1uqbmm"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import operator\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tópicos\n",
        "## LDA\n",
        "\n",
        "Link para baixar os dados: https://drive.google.com/drive/folders/1q7BQG_OdXkBEM_qE9BZ5eZu_EJzhOg1q?usp=drive_link"
      ],
      "metadata": {
        "id": "bdC1A-RwH0Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mypath = \"/content/drive/MyDrive/aulas/Processamento de Linguagem Natural - Saude/Modelos de NLP/chat/\"\n",
        "\n",
        "files = [f for f in listdir(mypath) if isfile(join(mypath, f)) and 'yml' in f]"
      ],
      "metadata": {
        "id": "d5qXfjGHHzti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files"
      ],
      "metadata": {
        "id": "LZug9saec6xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chats = {}\n",
        "for f in files:\n",
        "    print(\"file \", f)\n",
        "    with open(mypath+f, encoding=\"utf-8\") as file:\n",
        "        chat = yaml.safe_load(file)\n",
        "        all_chats[chat[\"categories\"][0]] = chat[\"conversations\"]"
      ],
      "metadata": {
        "id": "kR7EOUt_H7dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chats['games']"
      ],
      "metadata": {
        "id": "jnGq6d_2KpQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A variável ``chat`` é um dicionário, que possui a chave ``conversations``, que contém várias listas. Dentro de cada lista, existe um par de frases, uma escrita por um humano e outra escrita por um robô. Neste exercício, só iremos considerar a frase do humano, ou seja, a primeira posição do par nas listas.\n",
        "Exemplo:"
      ],
      "metadata": {
        "id": "cipXZK4DIBZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in all_chats.keys():\n",
        "    all_chats[k] = ' '.join([c[0] for c in all_chats[k]])"
      ],
      "metadata": {
        "id": "YVYypwPFICDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chats['games']"
      ],
      "metadata": {
        "id": "1oqhD0WPK6_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessamento"
      ],
      "metadata": {
        "id": "V-G6-sogIHNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_texto(corpus):\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    corpus_alt = [t for t in corpus_alt if t not in portugues_stops]\n",
        "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
        "\n",
        "    return corpus_alt"
      ],
      "metadata": {
        "id": "VJlucdwZIDtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "9s2MM1MaIJ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chats_clean = []\n",
        "for k in all_chats.keys():\n",
        "    all_chats_clean.append(pre_processamento_texto(all_chats[k]))"
      ],
      "metadata": {
        "id": "57nmG7FiILPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chats_clean[0]"
      ],
      "metadata": {
        "id": "fD_NNVgYen6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Criando um dicionario com o vocabulario\n",
        "\n",
        "dictionary = corpora.Dictionary(all_chats_clean)"
      ],
      "metadata": {
        "id": "Mb_g0tcbIN0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary[11]"
      ],
      "metadata": {
        "id": "wn-dwmetIPMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Convertendo o documento em indices\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in all_chats_clean]"
      ],
      "metadata": {
        "id": "5Yv2AnODITVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix,\n",
        "               id2word = dictionary,\n",
        "               num_topics=6,\n",
        "               passes=100,\n",
        "               random_state=42)"
      ],
      "metadata": {
        "id": "HwB9CvkXIZdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldamodel.show_topics()"
      ],
      "metadata": {
        "id": "YHeIimUTIcg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sumarização\n",
        "\n",
        "##Coleta do dado"
      ],
      "metadata": {
        "id": "k6PgagwBIdjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noticia_url = \"https://g1.globo.com/tecnologia/noticia/2022/03/12/instagram-restrito-na-russia-entenda-a-importancia-da-rede-social-para-o-pais-de-putin.ghtml\""
      ],
      "metadata": {
        "id": "8IE0akLtIgmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_class(target):\n",
        "    def do_match(tag):\n",
        "        classes = tag.get('class', [])\n",
        "        return all(c in classes for c in target)\n",
        "    return do_match\n",
        "\n",
        "def get_text_url(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    #remove marcações de scripts e style\n",
        "    texto = soup.find_all(match_class([\"content-text__container\"]))\n",
        "    all_text = \"\"\n",
        "    for t in texto:\n",
        "        all_text += t.get_text()\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "NiuZRUNWIh5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_noticia = get_text_url(noticia_url)"
      ],
      "metadata": {
        "id": "Ki01cgiNIjed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento"
      ],
      "metadata": {
        "id": "ST-zAOQzIs59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "corpus_sent = sent_tokenize(texto_noticia)"
      ],
      "metadata": {
        "id": "--G9Zkd4Il0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_processado = [pre_processamento_texto(sent) for sent in corpus_sent]"
      ],
      "metadata": {
        "id": "g-OPmg3gIolI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar a sumarização iremos criar os seguintes métodos;\n",
        "\n",
        "1. Um que calcula a similaridade de frases. Usando Bow\n",
        "2. Iremos construir uma matriz de similaridade\n",
        "3. Iremos fazer o rank das frases utilizando o método pagerank. Este método utiliza da representação de grafos então transformaremos nosso dado em um grafo.\n",
        "4. Ordene o score e retorne os 5 primeiros."
      ],
      "metadata": {
        "id": "vMQAJtGeIyLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_sentences(sent1, sent2):\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vect_bag = CountVectorizer(binary=False, analyzer=\"word\")\n",
        "    vect_bag.fit(all_words)\n",
        "    #aplica BOW\n",
        "    vec_sent1 = np.asarray(vect_bag.transform([' '.join(sent1)]).todense())\n",
        "    vec_sent2 = np.asarray(vect_bag.transform([' '.join(sent2)]).todense())\n",
        "\n",
        "    return cosine_similarity(vec_sent1, vec_sent2).reshape(-1)[0]"
      ],
      "metadata": {
        "id": "OZng8-wTJST3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_similarity(sentences):\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for ix in range(0,len(sentences)):\n",
        "        for ix2 in range(0,len(sentences)):\n",
        "            if ix == ix2:\n",
        "                continue\n",
        "            similarity_matrix[ix][ix2] = similarity_sentences(sentences[ix], sentences[ix2])\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "2zlfRiAmJTOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pagerank(texto):\n",
        "    sentences_matrix_similarity = matrix_similarity(texto)\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentences_matrix_similarity)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "snB0z9woJUYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(scores):\n",
        "    scores_sorted = dict(sorted(scores.items(), key=operator.itemgetter(1),reverse=True))\n",
        "    rank_sentences = list(scores_sorted.keys())[:5]\n",
        "    summarize_text = \"\"\n",
        "    for r in rank_sentences:\n",
        "        summarize_text += (corpus_sent[r]) + \" \"\n",
        "    return summarize_text"
      ],
      "metadata": {
        "id": "PtNIg1cmJVNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(get_pagerank(corpus_processado))"
      ],
      "metadata": {
        "id": "frovH8fjJWuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trocando para tfidf\n",
        "def similarity_sentences(sent1, sent2):\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vect = TfidfVectorizer()\n",
        "\n",
        "    vect.fit(all_words)\n",
        "    vec_sent1 = np.asarray(vect.transform([' '.join(sent1)]).todense())\n",
        "    vec_sent2 = np.asarray(vect.transform([' '.join(sent2)]).todense())\n",
        "\n",
        "    return cosine_similarity(vec_sent1, vec_sent2).reshape(-1)[0]"
      ],
      "metadata": {
        "id": "TSnoAwv9JYeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(get_pagerank(corpus_processado))"
      ],
      "metadata": {
        "id": "S7yuOflqJd2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}