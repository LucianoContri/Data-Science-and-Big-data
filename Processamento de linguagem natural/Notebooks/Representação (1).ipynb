{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "TfMtJgRJAC2s",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "302ed1bd-37c0-42ec-f940-0f7f087501f5"
   },
   "source": [
    "!pip install nltk==3.5\n",
    "!pip install gensim\n",
    "!pip install umap-learn\n",
    "!pip install wikipedia\n",
    "!pip install unidecode"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A6V8u14WAC26",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "37401288-c87b-424e-c1f6-2f5070049095"
   },
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import wikipedia\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import bz2\n",
    "import gensim\n",
    "import warnings\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2ZMw4zfAC3A"
   },
   "source": [
    "# Definição do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fumti3IlAC3B"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JhMedbmMAC3C"
   },
   "source": [
    "wikipedia.set_lang(\"pt\")\n",
    "bh = wikipedia.page(\"Belo Horizonte\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJ1WHOADAC3G"
   },
   "source": [
    "corpus = bh.content"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(corpus)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pn0mlRxfg6N5",
    "outputId": "7e11d31e-ba29-47d8-cf58-adf67b17fd18"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxnmUk2NAC3M"
   },
   "source": [
    "Selecionamos algumas frases do corpus de BH da wikipedia.\n",
    "\n",
    "Conside a lista abaixo como nosso corpus de documentos. Cada elemento da lista, considere como um único documento."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pd4pTiK0AC3M"
   },
   "source": [
    "documentos = \\\n",
    "[\"Belo Horizonte é um município brasileiro e a capital do estado de Minas Gerais\",\n",
    "\"A populacao de Belo Horizonte é estimada em 2 501 576 habitantes, conforme estimativas do Instituto Brasileiro de Geografia e Estatística\",\n",
    "\"Belo Horizonte já foi indicada pelo Population Crisis Commitee, da ONU, como a metrópole com melhor qualidade de vida na América Latina\",\n",
    "\"Belo Horizonte é mundialmente conhecida e exerce significativa influência nacional e até internacional, seja do ponto de vista cultural, econômico ou político\",\n",
    "\"Belo Horizonte é a capital do segundo estado mais populoso do Brasil, Minas Gerais\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1vjVVoYAC3T"
   },
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_MuzVllAC3U"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "1) Escreva uma método que realiza o pré-processamento da lista de <b>documentos</b>.\n",
    "\n",
    "O método deve, para cada documento:\n",
    "- tokenizar cada palavra\n",
    "- remover stopwords\n",
    "- remover números\n",
    "- remover pontuções\n",
    "- remover acentos"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFCuB_pCjhzM",
    "outputId": "b8ee8572-0e76-4ba1-d25c-f671ccf69b95"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "omIlYvYCcqXf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9fbyAieAC3j"
   },
   "source": [
    "# Representação Textual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFHut-GWAC4L"
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "Dica de leitura: https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XklQxnVKj7c\n",
    "\n",
    "Para representar o texto com TF-IDF utilizamos o TfidfVectorizer. A seguir apresentamos instruções sobre como utilizá-lo.\n",
    "\n",
    "```python\n",
    "#primeiro criamos o objeto\n",
    "vect = TfidfVectorizer()\n",
    "vect #aqui você pode observa todos os parâmetros que o objeto possui\n",
    "## Existem alguns parâmetros, opcionais, que podemos informar para uma possível melhora do nosso modelo. Por exemplo:\n",
    "### inclui 1-grams e 2-grams\n",
    "vect.set_params(ngram_range=(1, 2))\n",
    "### ignora termos que a aparecem em mais de 50% dos documentoss\n",
    "vect.set_params(max_df=0.5)\n",
    "### só considero termos que aparecem em ao menos 2 documentos\n",
    "vect.set_params(min_df=2)\n",
    "\n",
    "#depois aplicamos fit_transform para transformar o texto em números\n",
    "docs_tdidf = vect.fit_transform(docs)\n",
    "\n",
    "#o docs_tdidf é uma matriz com os números que representam cada um dos textos.\n",
    "## Conseguimos verificar a dimensão desta matriz:\n",
    "print(docs_tdidf.shape)\n",
    "\n",
    "#Para visualizar as features capturadas pelo TF-IDF utilize:\n",
    "print(vect.get_feature_names_out())\n",
    "#Para visualizar os vetores correspondentes a cada palavara utilize:\n",
    "df = pd.DataFrame(docs_tdidf.T.todense(), index=vect.get_feature_names_out(), columns=[\"doc\"+str(i+1) for i in range(0,len(docs))])\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZD0n75gAC4M"
   },
   "source": [
    "<b> Atividade: </b>\n",
    "\n",
    "2) Faça o TDIFTVectorizer nos documentos da variável <b>documentos</b> sem alterar nenhum parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ibe0yg9acsdS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbTCuJzIAC4o"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "3) Imprima o shape do resultado da atividade 2"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YuLngX0tctcq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQfYws4lAC4t"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "4) Imprima as features capturadas em 2."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ALTV329hctut"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kcdd6_IAC4x"
   },
   "source": [
    "5) Imprima os vetores correspondentes a cada palavra de cada documento."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZOyNJV9scvdV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF-ZevNeAC4-"
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "Para representar o bag of words utilizamos o CountVectorizer\n",
    "\n",
    "```python\n",
    "#primeiro criamos o objeto\n",
    "vect_bag = CountVectorizer(binary=False) #se binary = False -> ocorre a contagem da frequência em que a palavra aparece\n",
    "vect_bag #imprime os parâmetros\n",
    "\n",
    "```\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "6) Faça o CountVectorizer nos documentos da variável <b>documentos</b> considerando binary = True"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-eYFOa2GcxAu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0ZNTXLWAC5I"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "7) Imprima o índice correspondente a cada token da lista retornada por vect_bag.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qjhfvatgcx2C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbLt5-LiAC5v"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEGWYtn4AC7P"
   },
   "source": [
    "### Treinando seu embedding\n",
    "\n",
    "Aqui vamos utilizar o corpus machado. São textos/contos escritos por Machado de Assis.\n",
    "Esse corpus é diponibilizado pelo NLTK."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rdmp3F3OTl0B"
   },
   "source": [
    "from nltk.corpus import machado"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PDebqiPkLE3h",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4b98aaa6-3219-4506-bad2-76dcc7e18047"
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import machado\n",
    "nltk.download('machado')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EXFpVSHcRACL"
   },
   "source": [
    "raw_casmurro = machado.raw('contos/macn001.txt')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHribhOLAC7m"
   },
   "source": [
    "O método ''machado_sents()'' retorna todo o texto quebrado pelas setenças e já tokenizado.\n",
    "\n",
    "As sentenças são separadas pelo \"\\n\". Dentro de cada sentença, divide os tokens separadas pelo espaço."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r15rrg-qAC7m",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f1f508f0-0f5e-4a4f-8139-9ccca5004401"
   },
   "source": [
    "machado_sents = machado.sents()\n",
    "print(machado_sents)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b7kbw4m0tGoc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2fd7effb-3fb6-40eb-c7ce-4fb6c1c5c4e1"
   },
   "source": [
    "len(machado_sents)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek1UIrtOAC7s"
   },
   "source": [
    "Vamos relizar um pré-processamento mínimo nos dados. Lembrando que: o pré-processamento é impotatíssimo no resultado final.\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "8) Aplique as técnicas abaixo no documento <b> machado_sents</b>:\n",
    "\n",
    "- lower\n",
    "- remoção pontuações"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ECojZioZc-kv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhJKgQwTAC72"
   },
   "source": [
    "<b> Treinando o embedding </b>\n",
    "\n",
    "Para treinar os embeddings existem alguns parâmetros, vide exemplo abaixo:\n",
    "\n",
    "```python\n",
    "#Alguns parâmetros:\n",
    "## size -> dimensão vetor\n",
    "## min_count -> ignora todas palavras cuja frequência mínima é menor que este\n",
    "## workers -> quantas threads serão utilizadas para treinar o modelo\n",
    "## seed -> seed para geração do numero aleatório.\n",
    "## sg -> 1 para skip-gram; caso contrário CBOW.\n",
    "## window -> contexto, Distância máxima entre a palavra atual e a prevista em uma frase. O default é 5.\n",
    "model = word2vec.Word2Vec(text_preproc, min_count=10, workers=4, seed=123, sg=1, vector_size=300, window=5)\n",
    "```\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "9) Gere os embeddings com o texto processado do documento de Machado de Assis.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "1sqXLR9Oc_rw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvhul62-AC79"
   },
   "source": [
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "10) Faça os itens abaixo:\n",
    "\n",
    "- Verifique o vetor de embeddings da variável \"dom\"\n",
    "- Verifique a similaridade entre \"mulher\" e \"homem\"\n",
    "- Verifique a similaridade entre \"dom\" e \"casmurro\""
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "HAv_Yw_RdDrq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMjXdQ13AC8P"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "Dada as seguintes palavras:\n",
    "\n",
    "foi, relógio, amor, raiva, brasil.\n",
    "\n",
    "11) Escreva um método que retorne uma lista com as 5 palavras similares de cada uma das listadas anteriormente.\n",
    "Imprima a lista das palavras similares, incluindo a palavra origem."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Udt7RNiFdFUZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX5yNDdNAC8n"
   },
   "source": [
    "### Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_8sL0sOAC8o"
   },
   "source": [
    "Para a visualização dos embeddings iremos  construir um array com todas as palavras retornadas anteriormente.\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "12) Primeiro, gere uma única lista com todas as palavras retornadas anteriomente. O array deve ter tamanho 30."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5TaIxwEedGIX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhO4tuvyAC8w"
   },
   "source": [
    "13) Gere um array com todos os embeddings das palavras anteriores. Este terá dimensão (30,300)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pKm673SQdIFh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YjvhwhEAC85"
   },
   "source": [
    "<b>Dica: </b> Use a função abaixo para plotar o array 2D que será gerado com o método PCA e TSNE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "stqEUCMSAC86"
   },
   "source": [
    "def plot_embedding_2d(array_2d, all_words, words_seed):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    for (x, y), w in zip(array_2d, all_words):\n",
    "        ax.scatter(x, y, c='red' if w in words_seed else 'blue')\n",
    "        ax.annotate(w,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPvF4mqEAC89"
   },
   "source": [
    "#### PCA\n",
    "\n",
    "<b> É uma ténica que existe a mais de século. É rápido, determinístico e linear. Essa linearidade limita sua utilidade em domínios complexos, como linguagem natural ou imagens, onde a estrutura não linear. </b>\n",
    "\n",
    "Mais informações: https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-principal-component-analysis-1bdc77462831\n",
    "\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "14) Gere a visualização dos embeddings anteriores utilizando o PCA para reduzir a dimensionalidade.\n",
    "\n",
    "Exemplo do PCA:\n",
    "\n",
    "```python\n",
    "#uso de PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(array_embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "4yVHxdyXdL-u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiydMeXhAC9G"
   },
   "source": [
    "#### TSNE\n",
    "<b> Uma técnica mais recente que captura estrutura não linear é o t-SNE, que significa distribuição estocástica de embedding viziznhos em t ( t-distributed Stochastic Neighbor Embedding).\n",
    "É uma ótima técnica para capturar a estrutura não linear em dados de alta dimensão(pelo menos em nível local). Isto é, dois pontos que são próximos no espaço de alta dimensão a probabilidade de estarem próximos em uma dimensão baixa é alta. </b>\n",
    "\n",
    "Mais informações: https://medium.com/@garora039/dimensionality-reduction-using-t-sne-effectively-cabb2cd519b\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "15) Gere a visualização dos embeddings anteriores utilizando o TSNE para reduzir a dimensionalidade.\n",
    "\n",
    "Exemplo do TSNE:\n",
    "\n",
    "```python\n",
    "#uso de TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_result =  tsne.fit_transform(array_embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WbX7gwnvdRYi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Similaridade de Documentos\n",
    "\n",
    "Para realizar a similaridade entre documentos, utilize as frases abaixo:"
   ],
   "metadata": {
    "id": "R09dCQ0B7R9J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "frase1 = \"Excelente produto chegou antes do prazo indico e recomendo produto bom pois já testei e foi mais que aprovado\"\n",
    "frase2 = \"SUPER RECOMENDO, PREÇO, QUALIDADE #BRASTEMP, EFICIÊNCIA NA ENTREGA, E FACILIDADE DE PAGAMENTO. MUITO BOM!!!\"\n",
    "frase3 = \"A tampa do fogão veio com problemas com o pino de encaixe solto e precisa de reparos\"\n",
    "frase4 = \"Fogão ótimo!\""
   ],
   "metadata": {
    "id": "4SzWSYti7VIp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similaridade de Jaccard\n",
    "\n",
    "Divisão do número de tokens em comuns nas duas frases pelo tamanho do vocabulario das duas frases.\n",
    "\n",
    "<b> Atividade </b>\n",
    "\n",
    "16) Faça um método que calcule a similaridade de Jaccard e aplique para os seguintes pares de frases:\n",
    "\n",
    "- Frase1 e Frase2\n",
    "- Frase1 e Frase3\n",
    "- Frase2 e Frase3\n",
    "- Frase1 e Frase4"
   ],
   "metadata": {
    "id": "QyarEIOu7V3P"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PDxIxyV5dUAu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distância e similaridade cosseno\n",
    "\n",
    "Aqui iremos calcular a distância e a similaridade utilizando embedding\n",
    "\n",
    "- Embedding"
   ],
   "metadata": {
    "id": "1fIAlq397s4I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def embedding_da_frase(tokens_frase):\n",
    "  return np.mean(np.array([word_vectors[token] for token in tokens_frase if token in word_vectors.key_to_index]), axis=0)"
   ],
   "metadata": {
    "id": "3gFBOVa_70Yh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "frase1_emb = embedding_da_frase(frase1_pre)\n",
    "frase2_emb = embedding_da_frase(frase2_pre)"
   ],
   "metadata": {
    "id": "i7y6O6qmA7I4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "distance.cosine(frase1_emb, frase2_emb)"
   ],
   "metadata": {
    "id": "kVzXVgPkAz9e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cosine_similarity(frase1_emb.reshape((1, 100)), frase2_emb.reshape((1, 100)))"
   ],
   "metadata": {
    "id": "A6_gpFnIA5y2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "distance.euclidean(frase1_emb, frase2_emb)"
   ],
   "metadata": {
    "id": "Ifm_bFbuA8oL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
