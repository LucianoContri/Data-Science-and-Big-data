{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4HRgZdobKQ8iUPmfgKkaf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#LSTM\n","\n","**Objetivo**: Criação de um modelo utilizando arquitetura LSTM\n","\n","Para a criação de um modelo com arquitetura LSTM (Long short term memory) iremos utilizar o pacote tensorflow.\n","\n","1. Carregamento dos dados\n","\n","Batch: conjunto de observações que será passada por vez na rede neural, diferente dos modelos usuais de machine learning para redes neurais passamos um pequeno conjunto de dados a cada iteração."],"metadata":{"id":"f2q81wuxlu8a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U775PKnRlrJ6"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["dataset = tfds.load('imdb_reviews', as_supervised=True)\n","\n","train_dataset, test_dataset = dataset['train'], dataset['test']\n","\n","batch_size = 128\n","train_dataset = train_dataset.shuffle(15)\n","train_dataset = train_dataset.batch(batch_size)\n","test_dataset = test_dataset.batch(batch_size)"],"metadata":{"id":"-NKeTn4Sl6m2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Camada encoder\n","A primeira camada da nossa rede neural será a camada de enconder, essa camada irá converter as palavras para indexes. Para isso usaremos a função TextVectorization.\n","\n","```python\n","import tensorflow as tf\n","encoder = tf.keras.layers.TextVectorization()\n","encoder.adapt(train_dataset.map(lambda text, _: text))\n","```\n","\n","Com essa função iremos criar um vetor codificado das palavras do texto. O parâmetro max_tokens controla o tamanho do vocabulário na vetorização. A função adapt irá adequar o vocabulário do texto.\n","\n","Exercício:\n","1. Crie a camada encoder\n","2. Veja como fica um exemplo de texto na camada encoder (use encoder(texto).numpy())"],"metadata":{"id":"AuZzA4jGopR3"}},{"cell_type":"code","source":[],"metadata":{"id":"D8JQG5AS97to"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Criação do modelo\n","\n","Iremos criar uma arquitetura sequencial que seguirá a seguinte ordem:\n","\n","1. Encoder\n","2. Camada de embedding\n","3. Camada bidirecional\n","4. Camada Densa com ativação relu\n","5. Camada Densa com ativação linear\n","\n","\n","Para a construção do modelo iremos usar\n","```python\n","model = tf.keras.Sequential\n","model.add(camada)\n","```\n","\n","Onde em camada iremos substituir pelas camadas desejadas, por exemplo na camada de encoder usamos\n","```python\n","model.add(encoder)\n","```\n","\n","Para a criação da camada de embedding usaremos a função tf.keras.layers.Embedding, esta função possui dois parâmetros importantes\n","1. Input dimension - dimensão da entrada dos dados\n","2. Output dimension - dimensão da saída dos dados\n","Outro parâmetro importante é mask_zero que deve ser configurado como True caso as frases sejam de tamanho diferente.\n","\n","Exemplo de uso:\n","```python\n","tf.keras.layers.Embedding(10000, 32, mask_zero=True)\n","```\n","\n","Neste exemplo o input é de dimensão 10000 e o output de dimensão 32;\n","\n","Para a criação da camada BiLSTM usaremos a função tf.keras.layers.Bidirectional e a função tf.keras.layers.LSTM. Iremos configurar apenas a camada LSTM, nela temos que colocar o tamanho do output e um dos seus parâmetros é return_sequences que deve ser configurado como True caso desejamos o output de toda a frase e não apenas o último (default é falso).\n","\n","```python\n","tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(32,  return_sequences=True))\n","```\n","Ira criar a camada LSTM com saída 32 e irá retorna toda a sequencia.\n","\n","Por último a camada densa que é a camada mais simples, nela iremos passar o tamanho do output e a função de ativação; exemplos de função de ativação:\n","\n","* linear\n","* softmax (sigmoid)\n","* relu (max(0,x))\n","\n","```python\n","tf.keras.layers.Dense(64, activation='relu')\n","```\n","\n","Exercício construa o modelo com a seguinte arquitetura:\n","1. Encoder\n","2. Camada de embedding com output 64 e mask_zero=True\n","3. Camada BiLSTM, com output 64\n","4. Camada Densa com output 64 e ativação relu\n","5. Camada Densa com output 1"],"metadata":{"id":"fQGeEHQZppKb"}},{"cell_type":"code","source":[],"metadata":{"id":"QvYwPOzX9_qy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Treino\n","\n","Para resumir a arquitetura usamos a função summary\n","```python\n","model.summary()\n","```\n","\n","Após montar a arquitetura precisamos definir a função de perda, o otimizador e uma métrica de acompanhamento. Para isso usamos a função compile no nosso exemplo usaremos a função de perda de Crossentropy Binaria e o otimizador Adam como no exemplo.\n","\n","```python\n","model.compile(\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(),\n","    metrics=['accuracy']\n",")\n","```\n"," E para ajustar o modelo iremos usar .fit passando o dado de treino, o número de epochs (quantas vezes iremos passar o dado para a rede) e o banco de validação.\n","\n","```python\n","model.fit(\n","    train_dataset,  \n","    epochs=5,\n","    validation_data=test_dataset,\n",")\n","```\n","\n","Exercício:\n","1. Sumarize o modelo\n","2. Compile o modelo como no exemplo\n","3. Ajuste o modelo. Obs.: ao ajustar o modelo atribua o ajuste a um objeto."],"metadata":{"id":"Uqf_c0tKsp8G"}},{"cell_type":"code","source":[],"metadata":{"id":"QAGsTEqV-B5F"},"execution_count":null,"outputs":[]}]}