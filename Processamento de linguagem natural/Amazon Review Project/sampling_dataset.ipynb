{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T15:49:54.054984Z",
     "start_time": "2024-07-03T15:49:52.511226Z"
    }
   },
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:50:06.235221Z",
     "start_time": "2024-07-03T15:49:54.652294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ddf = dd.read_parquet('train.parquet')\n",
    "test_ddf = dd.read_parquet('test.parquet')\n",
    "\n",
    "# pegando sample dos dados\n",
    "train_df = train_ddf.sample(frac=0.02, random_state=42).compute()\n",
    "test_df = test_ddf.sample(frac=0.02, random_state=42).compute()\n",
    "\n",
    "train_df.to_parquet('train_sample.parquet') \n",
    "test_df.to_parquet('test_sample.parquet')"
   ],
   "id": "dc6148a07aa594e7",
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Failed to open local file 'train_sample.parquet'. Detail: [Windows error 5] Access is denied.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m train_df \u001B[38;5;241m=\u001B[39m train_ddf\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\u001B[38;5;241m.\u001B[39mcompute()\n\u001B[0;32m      6\u001B[0m test_df \u001B[38;5;241m=\u001B[39m test_ddf\u001B[38;5;241m.\u001B[39msample(frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\u001B[38;5;241m.\u001B[39mcompute()\n\u001B[1;32m----> 8\u001B[0m train_df\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_sample.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m) \n\u001B[0;32m      9\u001B[0m test_df\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_sample.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:2976\u001B[0m, in \u001B[0;36mDataFrame.to_parquet\u001B[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001B[0m\n\u001B[0;32m   2889\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2890\u001B[0m \u001B[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001B[39;00m\n\u001B[0;32m   2891\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2972\u001B[0m \u001B[38;5;124;03m>>> content = f.read()\u001B[39;00m\n\u001B[0;32m   2973\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2974\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparquet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_parquet\n\u001B[1;32m-> 2976\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m to_parquet(\n\u001B[0;32m   2977\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2978\u001B[0m     path,\n\u001B[0;32m   2979\u001B[0m     engine,\n\u001B[0;32m   2980\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   2981\u001B[0m     index\u001B[38;5;241m=\u001B[39mindex,\n\u001B[0;32m   2982\u001B[0m     partition_cols\u001B[38;5;241m=\u001B[39mpartition_cols,\n\u001B[0;32m   2983\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2984\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2985\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:430\u001B[0m, in \u001B[0;36mto_parquet\u001B[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001B[0m\n\u001B[0;32m    426\u001B[0m impl \u001B[38;5;241m=\u001B[39m get_engine(engine)\n\u001B[0;32m    428\u001B[0m path_or_buf: FilePath \u001B[38;5;241m|\u001B[39m WriteBuffer[\u001B[38;5;28mbytes\u001B[39m] \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mBytesIO() \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m path\n\u001B[1;32m--> 430\u001B[0m impl\u001B[38;5;241m.\u001B[39mwrite(\n\u001B[0;32m    431\u001B[0m     df,\n\u001B[0;32m    432\u001B[0m     path_or_buf,\n\u001B[0;32m    433\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m    434\u001B[0m     index\u001B[38;5;241m=\u001B[39mindex,\n\u001B[0;32m    435\u001B[0m     partition_cols\u001B[38;5;241m=\u001B[39mpartition_cols,\n\u001B[0;32m    436\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    438\u001B[0m )\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    441\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, io\u001B[38;5;241m.\u001B[39mBytesIO)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:204\u001B[0m, in \u001B[0;36mPyArrowImpl.write\u001B[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mwrite_to_dataset(\n\u001B[0;32m    196\u001B[0m             table,\n\u001B[0;32m    197\u001B[0m             path_or_handle,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    200\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    201\u001B[0m         )\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;66;03m# write to single output file\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mwrite_table(\n\u001B[0;32m    205\u001B[0m             table, path_or_handle, compression\u001B[38;5;241m=\u001B[39mcompression, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    206\u001B[0m         )\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:3071\u001B[0m, in \u001B[0;36mwrite_table\u001B[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001B[0m\n\u001B[0;32m   3069\u001B[0m use_int96 \u001B[38;5;241m=\u001B[39m use_deprecated_int96_timestamps\n\u001B[0;32m   3070\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3071\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ParquetWriter(\n\u001B[0;32m   3072\u001B[0m             where, table\u001B[38;5;241m.\u001B[39mschema,\n\u001B[0;32m   3073\u001B[0m             filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m   3074\u001B[0m             version\u001B[38;5;241m=\u001B[39mversion,\n\u001B[0;32m   3075\u001B[0m             flavor\u001B[38;5;241m=\u001B[39mflavor,\n\u001B[0;32m   3076\u001B[0m             use_dictionary\u001B[38;5;241m=\u001B[39muse_dictionary,\n\u001B[0;32m   3077\u001B[0m             write_statistics\u001B[38;5;241m=\u001B[39mwrite_statistics,\n\u001B[0;32m   3078\u001B[0m             coerce_timestamps\u001B[38;5;241m=\u001B[39mcoerce_timestamps,\n\u001B[0;32m   3079\u001B[0m             data_page_size\u001B[38;5;241m=\u001B[39mdata_page_size,\n\u001B[0;32m   3080\u001B[0m             allow_truncated_timestamps\u001B[38;5;241m=\u001B[39mallow_truncated_timestamps,\n\u001B[0;32m   3081\u001B[0m             compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   3082\u001B[0m             use_deprecated_int96_timestamps\u001B[38;5;241m=\u001B[39muse_int96,\n\u001B[0;32m   3083\u001B[0m             compression_level\u001B[38;5;241m=\u001B[39mcompression_level,\n\u001B[0;32m   3084\u001B[0m             use_byte_stream_split\u001B[38;5;241m=\u001B[39muse_byte_stream_split,\n\u001B[0;32m   3085\u001B[0m             column_encoding\u001B[38;5;241m=\u001B[39mcolumn_encoding,\n\u001B[0;32m   3086\u001B[0m             data_page_version\u001B[38;5;241m=\u001B[39mdata_page_version,\n\u001B[0;32m   3087\u001B[0m             use_compliant_nested_type\u001B[38;5;241m=\u001B[39muse_compliant_nested_type,\n\u001B[0;32m   3088\u001B[0m             encryption_properties\u001B[38;5;241m=\u001B[39mencryption_properties,\n\u001B[0;32m   3089\u001B[0m             write_batch_size\u001B[38;5;241m=\u001B[39mwrite_batch_size,\n\u001B[0;32m   3090\u001B[0m             dictionary_pagesize_limit\u001B[38;5;241m=\u001B[39mdictionary_pagesize_limit,\n\u001B[0;32m   3091\u001B[0m             store_schema\u001B[38;5;241m=\u001B[39mstore_schema,\n\u001B[0;32m   3092\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m writer:\n\u001B[0;32m   3093\u001B[0m         writer\u001B[38;5;241m.\u001B[39mwrite_table(table, row_group_size\u001B[38;5;241m=\u001B[39mrow_group_size)\n\u001B[0;32m   3094\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:984\u001B[0m, in \u001B[0;36mParquetWriter.__init__\u001B[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **options)\u001B[0m\n\u001B[0;32m    979\u001B[0m         sink \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_handle \u001B[38;5;241m=\u001B[39m filesystem\u001B[38;5;241m.\u001B[39mopen(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    980\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m         \u001B[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001B[39;00m\n\u001B[0;32m    982\u001B[0m         \u001B[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001B[39;00m\n\u001B[0;32m    983\u001B[0m         \u001B[38;5;66;03m# shouldn't implicitly apply compression.\u001B[39;00m\n\u001B[1;32m--> 984\u001B[0m         sink \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_handle \u001B[38;5;241m=\u001B[39m filesystem\u001B[38;5;241m.\u001B[39mopen_output_stream(\n\u001B[0;32m    985\u001B[0m             path, compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    986\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    987\u001B[0m     sink \u001B[38;5;241m=\u001B[39m where\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_fs.pyx:868\u001B[0m, in \u001B[0;36mpyarrow._fs.FileSystem.open_output_stream\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:144\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:111\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mPermissionError\u001B[0m: [WinError 5] Failed to open local file 'train_sample.parquet'. Detail: [Windows error 5] Access is denied.\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "af3da7cec67b1dd9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
