{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Aplicações de Dados\n",
    "\n",
    "Alunos: \n",
    "\n",
    "        Eduardo Junior de Almeida - 1311267\n",
    "        Luciano Augusto Scherer Contri - 1500596\n",
    "        Ricardo Romano Langaro - 197829\n",
    "        Rodrigo Góis Santiago - 188982\n",
    "        Winclatis Filipe Costa - 189227\n",
    "\n",
    "Objetivo:\n",
    " \n",
    "- Utilizar técnicas de NLP para análise de sentimentos em reviews de produtos da Amazon.\n",
    "- treinar modelos de Machine Learning para classificar os produtos como bons ou ruins, baseado nas avaliações dos clientes.\n",
    "- Avaliar a acurácia dos modelos de Machine Learning aplicados.\n",
    "\n",
    "\n",
    "\n",
    "**Descrição da Base de Dados:**\n",
    "\n",
    "| Campo             | Descrição                               |\n",
    "|-------------------|-----------------------------------------|\n",
    "| product_title     | Nome do Produto avaliado                |\n",
    "| average_rating    | Média das Avaliações em Notas           |\n",
    "| review_author     | Nome/Apelido do Usuário que realizou a Avaliação |\n",
    "| review_rating     | Nota da Avaliação do Produto            |\n",
    "| review_text       | Avaliação dos Clientes em Texto         |\n",
    "| group_sequence    | Código do Produto / Grupo de Reviews    |\n",
    "\n",
    "Dataset: amazon-customer-reviews-for-some-products (https://www.kaggle.com/datasets/mahran34/amazon-customer-reviews-for-some-products?resource=download)\n",
    "\n",
    "Divisão do notebook seguindo a metodologia CRISP-DM:\n",
    "- Preparação dos Dados\n",
    "- Modelagem\n",
    "- Avaliação\n",
    "- Conclusão"
   ],
   "id": "8cf16d6f4d742be8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparação dos Dados",
   "id": "571df80373b57edf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importação das Bibliotecas",
   "id": "7cfc01bdec053aca"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:26.556727Z",
     "start_time": "2024-06-27T18:35:25.671800Z"
    }
   },
   "source": "import pandas as pd",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:26.658722Z",
     "start_time": "2024-06-27T18:35:26.557729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('../../Datasets/df_review.csv', sep = ',')\n",
    "df_review"
   ],
   "id": "4140b11b3df45867",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Limpando feature review_text",
   "id": "fe050b88fec03227"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:26.676502Z",
     "start_time": "2024-06-27T18:35:26.659724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# quantidade de reviews unicos\n",
    "print('Quantidade de reviews únicos:', df_review['review_text'].nunique())\n",
    "\n",
    "# quantidade de reviews duplicados\n",
    "print('Quantidade de reviews duplicados:', df_review.duplicated().sum())\n",
    "\n",
    "# quantidade de reviews nulos\n",
    "print('Quantidade de reviews nulos:', df_review['review_text'].isnull().sum())\n",
    "\n",
    "# quantidade de reviews vazios  \n",
    "print('Quantidade de reviews vazios:', df_review[df_review['review_text'] == ''].shape[0])\n"
   ],
   "id": "118f61fa201871f7",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:26.704815Z",
     "start_time": "2024-06-27T18:35:26.677505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# normalizar os textos para minúsculo e remover espaços em branco no início e no final  \n",
    "df_review['review_text'] = df_review['review_text'].str.lower().str.strip()\n",
    "\n",
    "# Remover duplicatas e valores nulos\n",
    "df_review = (df_review.drop_duplicates()\n",
    "                      .dropna(subset=['review_text']))\n"
   ],
   "id": "942163212d1dfa4e",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:26.718735Z",
     "start_time": "2024-06-27T18:35:26.705817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mostrar dados duplicados  \n",
    "df_review[df_review.duplicated(subset=['review_text'], keep=False)]"
   ],
   "id": "be4979f54a04ed63",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Classificar linguas com langdetect\n",
    "\n",
    "Como podemos observar a nossa feature review_text é a que contém as avaliações dos clientes, e é a que será utilizada para a análise de sentimentos. No entanto, os textos se encontram em linguas diferentes, o que pode dificultar a análise. Para resolver isso, podemos traduzir os textos para inglês utilizando a biblioteca googletrans ou classificar as linguas e separar os textos por lingua. Para isso, utilizaremos a biblioteca langdetect.\n",
    "Pois a biblioteca googletrans é uma API e tem um custo para traduzir os textos. além de demorar mais tempo para processar."
   ],
   "id": "9360395530b6baf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:59.988590Z",
     "start_time": "2024-06-27T18:35:26.719739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# usar langdetect para detectar a lingua dos textos\n",
    "from langdetect import detect\n",
    "\n",
    "# função para detectar a lingua dos textos\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "# detectar a lingua dos textos\n",
    "df_review['lang'] = df_review['review_text'].apply(detect_lang)\n",
    "\n",
    "# mostrar a quantidade de textos por lingua\n",
    "df_review['lang'].value_counts()"
   ],
   "id": "f9851e8d30d0873b",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:35:59.994710Z",
     "start_time": "2024-06-27T18:35:59.989593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop textos que não estão em inglês\n",
    "df_review = df_review[df_review['lang'] == 'en']"
   ],
   "id": "fc9bc42eb942055b",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reduzimos de 8000 unicos para 7000 unicos após a remoção dos textos que não estão em inglês. Infelizmente é custoso usar APIs para traduzir como google cloud translation. Mas continua boa a quantidade de dados para treinar os modelos.",
   "id": "df189166baf60d55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agora como os ratings vão de 1 a 5, vamos considerar que ratings 1 e 2 são negativo, 3 é neutro e 4 e 5 são positivo. Vamos criar uma nova coluna target. ",
   "id": "aa747a67551a0cdc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:00.001504Z",
     "start_time": "2024-06-27T18:35:59.995712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# criar a coluna Target\n",
    "df_review['target'] = df_review['review_rating'].apply(lambda x: 'neg' if x <= 3 else 'pos')"
   ],
   "id": "440b38ebf9f7abc6",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:00.091971Z",
     "start_time": "2024-06-27T18:36:00.002512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# salvar o dataframe\n",
    "df_review.to_csv('df_review_clean.csv', index=False)"
   ],
   "id": "5464cba0e6566a24",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb48a058872d15cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "365a9c39fe264a22",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "train = pd.read_csv('../../Datasets/Amazon Reviews/train.csv', sep = ',', header= None)",
   "id": "8b4061a25ef2329f",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:27:50.545338Z",
     "start_time": "2024-07-01T22:27:50.538780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train.columns = ['rating', 'review_title', 'review_text']\n",
    "train"
   ],
   "id": "e3ed45eae73ec89",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "test = pd.read_csv('../../Datasets/Amazon Reviews/test.csv', sep = ',', header = None)",
   "id": "cf9b261583dc6cea",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:27:29.551006Z",
     "start_time": "2024-07-01T22:27:29.544922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test.columns = ['rating', 'review_title', 'review_text'] \n",
    "test"
   ],
   "id": "349e220d69bcff8f",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modelagem\n",
    "\n",
    "Nessa etapa, vamos treinar modelos de Machine Learning tradicionais e modelos de Deep Learning para classificar os produtos como bons ou ruins, baseado nas avaliações dos clientes. Visa-se avaliar a acurácia dos modelos aplicados e comparar os resultados."
   ],
   "id": "df143215a0541a84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### tokenização e remoção de stopwords de textos em inglês",
   "id": "4f45f580328674fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:07.335684Z",
     "start_time": "2024-06-27T18:36:00.093974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import unidecode\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Função para preprocessar texto\n",
    "def preprocess_text_traditional(text):\n",
    "    # Inicializar lematizador\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = unidecode.unidecode(text) # Remover acentos   \n",
    "    text = text.lower() # Converter para minúsculas    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))# Remover pontuação    \n",
    "    words = word_tokenize(text)# Tokenizar    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]# Remover stopwords    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words]# Lematizar   \n",
    "    return ' '.join(words) # Juntar tokens de volta em uma string para modelos tradicionais\n",
    "\n",
    "X_text = df_review['review_text'].apply(lambda x: preprocess_text_traditional(x))\n"
   ],
   "id": "cf0be38a40f5362b",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelos de Machine Learning Tradicionais",
   "id": "ebf29c148cc9f059"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Função para treinar modelos tradicionais, salvar avaliações e salvar modelo ",
   "id": "63c6a833efd8ffb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:07.341624Z",
     "start_time": "2024-06-27T18:36:07.336687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import train_test_split    \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# função para treinar modelo tradicionais, salvar avaliações e salvar modelo \n",
    "def train_model(df, model_type, X_text):\n",
    "    \n",
    "    # Preprocessar texto\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    # Codificar o target\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['target'])\n",
    "    \n",
    "    # Dividir dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    model = model_type\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Prever valores    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Avaliar modelo    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # salvar modelo\n",
    "    joblib.dump(model, f'{model_type}.pkl')\n",
    "    \n",
    "    return report, accuracy"
   ],
   "id": "7ea8806947c87592",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Treinamento e avaliação dos modelos tradicionais    ",
   "id": "4af7c758f52d4b07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:22.426496Z",
     "start_time": "2024-06-27T18:36:07.342627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Inicializar modelos\n",
    "logistic = LogisticRegression() \n",
    "random_forest = RandomForestClassifier()    \n",
    "svc = SVC()\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "# Treinar modelos\n",
    "report_logistic, accuracy_logistic = train_model(df_review, logistic, X_text)\n",
    "report_random_forest, accuracy_random_forest = train_model(df_review, random_forest, X_text)\n",
    "report_svc, accuracy_svc = train_model(df_review, svc, X_text)\n",
    "report_naive_bayes, accuracy_naive_bayes = train_model(df_review, naive_bayes, X_text)\n"
   ],
   "id": "50c7176fcdbf5103",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:22.431723Z",
     "start_time": "2024-06-27T18:36:22.427497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print('Logistic Regression')\n",
    "print('Accuracy:', accuracy_logistic)\n",
    "print(report_logistic)\n",
    "print('-'*50)\n",
    "\n",
    "print('Random Forest')\n",
    "print('Accuracy:', accuracy_random_forest)\n",
    "print(report_random_forest)\n",
    "print('-'*50)\n",
    "\n",
    "print('SVC')\n",
    "print('Accuracy:', accuracy_svc)\n",
    "print(report_svc)\n",
    "print('-'*50)\n",
    "\n",
    "print('Naive Bayes')\n",
    "print('Accuracy:', accuracy_naive_bayes)    \n",
    "print(report_naive_bayes)\n"
   ],
   "id": "16731498802939a9",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Modelos de Deep Learning",
   "id": "3e20267cfc88fbf3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LSTM",
   "id": "6d9e477bf78f315d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:25.888314Z",
     "start_time": "2024-06-27T18:36:22.431723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics\n",
    "import tensorflow as tf     \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scikeras\n"
   ],
   "id": "da8cdd829c2753e3",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tokenização e Padronização para LSTM  ",
   "id": "f9c524255ecfa8c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:26.169747Z",
     "start_time": "2024-06-27T18:36:25.888314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializar tokenizador específico para LSTM\n",
    "tokenizer_lstm = Tokenizer(num_words=5000)\n",
    "\n",
    "# Tokenização e Padronização\n",
    "tokenizer_lstm.fit_on_texts(X_text)\n",
    "vocab_size = len(tokenizer_lstm.word_index) + 1\n",
    "print('Vocab Size:', vocab_size)\n",
    "X = tokenizer_lstm.texts_to_sequences(X_text)\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Encode target labels to one-hot\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_review['target'])\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=2)  # Convert to one-hot encoding\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "6a45013ed875b4ca",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Função para criar modelo LSTM ",
   "id": "a6763199c65b4de0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:26.177580Z",
     "start_time": "2024-06-27T18:36:26.170751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para criar modelo LSTM\n",
    "def create_model(dropout_rate= 0.5, LSTM_layers= 1, LSTM_neurons= 64, dense_layers= 1, dense_neurons= 32, activation= 'relu'):\n",
    "    try:\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
    "        \n",
    "        \n",
    "        if LSTM_layers == 3:\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        if LSTM_layers == 2:\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        \n",
    "        model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=False)))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        for _ in range(dense_layers-1):\n",
    "            model.add(Dense(dense_neurons, activation=activation, kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(Dense(dense_neurons, activation=activation, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dense(2, kernel_initializer='uniform', activation='softmax'))  # Adjusted this line\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while adding layers: {e}\")\n",
    "        return None"
   ],
   "id": "b89fd129ba28ecb4",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Random Search com 50 iterações",
   "id": "14a5a9a89a8ba712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:36:26.184095Z",
     "start_time": "2024-06-27T18:36:26.178583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# \n",
    "# # Define the hyperparameters to tune and their possible values\n",
    "# param_grid = {\n",
    "#     'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "#     'LSTM_layers': [1, 2, 3],\n",
    "#     'LSTM_neurons': [32, 64, 128],\n",
    "#     'dense_layers': [1, 2, 3],\n",
    "#     'dense_neurons': [32, 64, 128],\n",
    "#     'activation': ['relu', 'leaky_relu', 'elu'],\n",
    "#     'optimizer': [Adam, SGD, RMSprop, Adagrad],\n",
    "#     'learning_rate': [0.001, 0.01, 0.005],\n",
    "#     'batch_size': [16, 32, 64, 128],\n",
    "#     'epochs': [10]\n",
    "# }\n",
    "# \n",
    "# # Define the number of iterations for the random search\n",
    "# n_iter = 50\n",
    "# \n",
    "# best_score = 0\n",
    "# best_params = {}\n",
    "# best_report = None\n",
    "# \n",
    "# for i in range(n_iter):\n",
    "#     # Randomly sample parameters for this round\n",
    "#     params = {k: np.random.choice(v) for k, v in param_grid.items()}\n",
    "# \n",
    "#     # Create a new model with the sampled parameters\n",
    "#     model = create_model(dropout_rate=params['dropout_rate'],\n",
    "#                          LSTM_layers=params['LSTM_layers'],\n",
    "#                          LSTM_neurons=params['LSTM_neurons'],\n",
    "#                          dense_layers=params['dense_layers'],\n",
    "#                          dense_neurons=params['dense_neurons'],\n",
    "#                          activation=params['activation'])\n",
    "# \n",
    "#     if params['optimizer'] == Adam: \n",
    "#         optimizer = Adam(learning_rate=params['learning_rate']) \n",
    "#     elif params['optimizer'] == RMSprop:\n",
    "#         optimizer = RMSprop(learning_rate=params['learning_rate'])  \n",
    "#     elif params['optimizer'] == Adagrad:\n",
    "#         optimizer = Adagrad(learning_rate=params['learning_rate'])\n",
    "#     else:\n",
    "#         optimizer = SGD(learning_rate=params['learning_rate'])\n",
    "#         \n",
    "# \n",
    "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# \n",
    "#     # Define callbacks\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=2, min_lr=0.0001)\n",
    "#     \n",
    "#     print(f'Iteration {i+1}/{n_iter}')\n",
    "#     print(f'Parameters: {params}')\n",
    "#     # Train the model\n",
    "#     model.fit(X_train, \n",
    "#               y_train, \n",
    "#               epochs=params['epochs'], \n",
    "#               batch_size=params['batch_size'],\n",
    "#               validation_data=(X_test, y_test),\n",
    "#               verbose=2, \n",
    "#               callbacks=[early_stopping, reduce_lr])\n",
    "# \n",
    "#     # Evaluate the model\n",
    "#     y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "#     y_test_binary = np.argmax(y_test, axis=-1)  # Convert y_test to binary format\n",
    "#     score = accuracy_score(y_test_binary, y_pred)  # Use y_test_binary here\n",
    "#     # classification report\n",
    "#     report_lstm = classification_report(y_test_binary, y_pred, zero_division= 0)  # Use y_test_binary here\n",
    "#     print('Accuracy:', score)\n",
    "#     print(report_lstm)\n",
    "#     \n",
    "# \n",
    "#     # If the score for this model is better than our current best, update the best score and best parameters\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_params = params\n",
    "#         best_report = report_lstm\n",
    "# \n",
    "# # Print the best score and the parameters that produced it\n",
    "# print('Best score:', best_score)\n",
    "# print('Best parameters:', best_params)"
   ],
   "id": "b17e9dcdf64cc34f",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "50 iterações de uma random search com diferentes configurações de parametros e layers nos resultou em um modelo com:\n",
    "\n",
    "- Best score: 0.8689087165408373\n",
    "- Best parameters: {'dropout_rate': 0.5, 'LSTM_layers': 2, 'LSTM_neurons': 64, 'dense_layers': 1, 'dense_neurons': 32, 'activation': 'relu', 'optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 10}"
   ],
   "id": "3383d35db5cb5a77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Custom Callback   \n",
    "\n",
    "Criamos um custom callback para monitorar o treinamento do modelo e salvar os melhores pesos. De forma que eu posso escolher quantas épocas esperar para reduzir o learning rate e quantas épocas esperar para restaurar os melhores pesos. Então Criei uma Rotina dessa forma:\n",
    "\n",
    "- Se o modelo não melhorar por um número de épocas (patience) ele restaura os melhores pesos.\n",
    "- Se o modelo não melhorar por um número de épocas (lr_patience) ele reduz o learning rate. \n",
    "- Se o learning rate chegar no mínimo, ele para o treinamento.  \n",
    "\n",
    "Dessa forma a estrutura deixa o modelo re-testar o learning_rate no melhor ponto de mínimo local, e não deixa o modelo divagar.\n",
    "se mesmo assim aquele learning_rate não funcionar, ele vai diminuindo até chegar no mínimo e parar o treinamento. \n",
    "\n",
    "Também criei outra rotina onde ela:\n",
    "- salva os pesos e os resultados de 3 épocas que rodam com os mesmo pesos\n",
    "- compara se uma dessas 3 épocas melhorou o modelo.\n",
    "- Se Melhorou atualiza os pesos\n",
    "- Se piorou, ele reduz o learning rate e realiza mais 3 épocas\n",
    "- Se o learning_rate chegar no mínimo encerra-se o treinamento.\n",
    "\n",
    "Infelizmente o Callback ReduceLROnPlateau não funciona dessa forma, ele reduz o learning rate mas ele aplica nos pesos atuais que podem ter divagado muito longe, e não nos melhores pesos. então por exemplo, se com o learning rate de 0.01 o modelo piorar significativamente no periodo de pacience, ele reduzirá o learning rate para 0.004, mas continuará treinando com os pesos atuais que pioraram o modelo e estão em um minimo local pior. Por exemplo: ele no inicio do treinamento encontrou um mínimo local de val_loss de 0.4 7 epócas dentro do treinamento, mas nas outras interações ele divagou para 0.55, o ReduceLROnPlateau vai reduzir o learning rate para 0.001 mas o modelo vai continuar do local val_loss de 0.55 e com um learning_rate menor dificilmente irá achar um mínimo local melhor."
   ],
   "id": "1a1658547a9b9426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:27:39.028337Z",
     "start_time": "2024-06-28T00:27:39.017630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, patience=0, lr_patience=3, reduce_factor=0.5, min_lr=0.0001):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.lr_patience = lr_patience \n",
    "        self.wait = 0\n",
    "        self.lr_wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best_loss = np.inf\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.best_epoch = 0\n",
    "        self.results = []\n",
    "        self.weights = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.lr_wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best_loss = np.inf\n",
    "        self.results = []\n",
    "        self.weights = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        print(\"\\nLearning rate is %6.4f.\" % (lr))\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        # print(\"current wait count:\", self.wait)\n",
    "        print(\"best loss:\", self.best_loss)\n",
    "        print(\"current loss:\", logs.get(\"val_loss\"))\n",
    "        if self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(\"val_loss\")\n",
    "        self.results.append(current_loss)\n",
    "        self.weights.append(self.model.get_weights())\n",
    "        if self.best_weights is None:\n",
    "            self.best_weights = self.model.get_weights()  \n",
    "        if len(self.results) == 3:\n",
    "            min_loss_index = np.argmin(self.results)\n",
    "            min_loss = self.results[min_loss_index]\n",
    "            if min_loss < self.best_loss:\n",
    "                self.best_loss = min_loss\n",
    "                self.best_weights = self.weights[min_loss_index]\n",
    "            else:\n",
    "                old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "                new_lr = old_lr * self.reduce_factor\n",
    "                new_lr = max(new_lr, self.min_lr)\n",
    "                if new_lr == self.min_lr:\n",
    "                    self.model.stop_training = True\n",
    "                self.model.optimizer.learning_rate = tf.Variable(new_lr)\n",
    "            self.results = []\n",
    "            self.weights = []\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     current_loss = logs.get(\"val_loss\")\n",
    "    #     if np.less(current_loss, self.best_loss):\n",
    "    #         self.best_loss = current_loss\n",
    "    #         self.best_weights = self.model.get_weights()\n",
    "    #         self.best_epoch = epoch\n",
    "    #         self.wait = 0\n",
    "    #         self.lr_wait = 0\n",
    "    #     else:\n",
    "    #         self.wait += 1\n",
    "    #         self.lr_wait += 1\n",
    "    #         print(f\"\\nLoss did not improve. Current wait count: {self.wait}.\")\n",
    "    #         if self.wait >= self.patience:\n",
    "    #             print('restoring best weights') \n",
    "    #             self.model.set_weights(self.best_weights)\n",
    "    #             self.wait = 0\n",
    "    #         if self.lr_wait >= self.lr_patience:\n",
    "    #             print('reducing learning rate')\n",
    "    #             old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "    #             new_lr = old_lr * self.reduce_factor\n",
    "    #             new_lr = max(new_lr, self.min_lr)\n",
    "    #             if new_lr == self.min_lr:\n",
    "    #                 self.model.stop_training = True\n",
    "    #             self.model.optimizer.learning_rate = tf.Variable(new_lr)\n",
    "    #             self.lr_wait = 0\n",
    "                \n",
    "                \n",
    "# Instantiate the custom callback and use it as an argument in model.fit\n",
    "custom_callback = CustomCallback(patience=3,lr_patience=9)"
   ],
   "id": "29bbee1ab62402f9",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Treinamento com os melhores parametros encontrados na random search mas com um número maior de épocas.",
   "id": "b03c75fdb956fe4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:32:56.057998Z",
     "start_time": "2024-06-28T00:27:39.828961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar modelo com os melhores parametros\n",
    "model_lstm = create_model(dropout_rate=0.5, LSTM_layers=2, LSTM_neurons=64, dense_layers=1, dense_neurons=32, activation='relu')\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=1, min_lr=0.0001)\n",
    "# model_checkpoint = ModelCheckpoint('best_model_lstm_checkpoint.keras', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "# Treinar modelo\n",
    "model_lstm.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_test, y_test), verbose=2, callbacks=[custom_callback])\n",
    "\n",
    "# Restaurar melhores pesos\n",
    "model_lstm.set_weights(custom_callback.best_weights)\n",
    "\n",
    "# salvar modelo\n",
    "model_lstm.save('best_model_lstm_30_epochs.keras')\n",
    "\n",
    "# Avaliar modelo    \n",
    "y_pred = np.argmax(model_lstm.predict(X_test), axis=-1)\n",
    "y_test_binary = np.argmax(y_test, axis=-1)  # Convert y_test to binary format\n",
    "accuracy_lstm = accuracy_score(y_test_binary, y_pred)  # Use y_test_binary here\n",
    "# classification report\n",
    "report_lstm = classification_report(y_test_binary, y_pred, zero_division= 0)  # Use y_test_binary here\n"
   ],
   "id": "88a04e0302656b5e",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:38:13.960919Z",
     "start_time": "2024-06-28T00:38:13.956991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Accuracy:', accuracy_lstm)       \n",
    "print(report_lstm)\n"
   ],
   "id": "7fbaab5d07f74b3b",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6295df291d032998"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Avaliação\n",
    "\n",
    " Plotar as avaliações dos modelos de Machine Learning tradicionais e do modelo de Deep Learning LSTM.\n"
   ],
   "id": "e072a7a6125c1be9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:38:17.396560Z",
     "start_time": "2024-06-28T00:38:17.112344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Avaliações dos modelos\n",
    "models = ['Logistic Regression', 'Random Forest', 'SVC', 'Naive Bayes', 'LSTM'] \n",
    "accuracies = [accuracy_logistic, accuracy_random_forest, accuracy_svc, accuracy_naive_bayes, accuracy_lstm]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, accuracies, color='skyblue')    \n",
    "\n",
    "# numeros muito proximos, vamos usar escala logaritmica\n",
    "plt.yscale('log')\n"
   ],
   "id": "33bccc6b732be878",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Como podemos ver, os modelos são muito próximos, consegui uma melhor acurácia com o modelo LSTM, mas a diferença é muito pequena e o custo computacional é muito maior. A limitação da quantidade de dados influencia muito na acurácia de modelos de Deep Learning, e a quantidade de dados que temos é suficiente para modelos tradicionais. o que pode ser uma explicação para a diferença pequena de acurácia. Provavelmente com mais dados o modelo de Deep Learning teria uma vantagem maior. Mas com a quantia atual de dados, os modelos tradicionais são mais eficientes e com total certeza, se fosse feita uma busca de hiperparametros mais refinada na Random Forest, ela poderia superar com folga o LSTM.",
   "id": "9aa38dd5ddee90a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "6b78922a1bd0e6d2",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
