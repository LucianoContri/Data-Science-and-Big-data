{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:20:46.597257Z",
     "start_time": "2024-07-02T16:20:34.727944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import  dask.dataframe as dd\n",
    "\n",
    "# Carregar os dados\n",
    "# Read the Parquet files with Dask\n",
    "train_ddf = dd.read_parquet('train.parquet')\n",
    "test_ddf = dd.read_parquet('test.parquet')\n",
    "\n",
    "# pegando sample dos dados\n",
    "train_df = train_ddf.sample(frac=0.02, random_state=42).persist()\n",
    "test_df = test_ddf.sample(frac=0.02, random_state=42).persist()\n",
    "\n"
   ],
   "id": "58eb5a41cf3eb4d2",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:20:46.646572Z",
     "start_time": "2024-07-02T16:20:46.599264Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.shape, test_df.shape",
   "id": "612780c2dd83ef3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Delayed('int-651612d1-9a19-4213-af36-b8795a89a4a6'), 3),\n",
       " (Delayed('int-a8f90c8a-c7d6-4468-bd27-f4041dc2dc1a'), 3))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:20:46.703292Z",
     "start_time": "2024-07-02T16:20:46.648579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import gc\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adagrad\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf     \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import unidecode\n",
    "# Downloads necessários para o NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "id": "14886c083d8a2371",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lucia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lucia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lucia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tokenização e Padronização para LSTM  ",
   "id": "1059befd3acfe428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:20:46.729341Z",
     "start_time": "2024-07-02T16:20:46.704299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para preprocessar texto\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text_traditional(text):\n",
    "    text = unidecode.unidecode(text)  # Remove accents\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize\n",
    "    return ' '.join(words)  # Join tokens back into a string\n",
    "\n",
    "def preprocess_texts(df):\n",
    "    df['processed_text'] = df['review_text'].apply(preprocess_text_traditional, meta=('review_text', 'str'))\n",
    "    return df"
   ],
   "id": "dacea5faf2320058",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:21:27.279660Z",
     "start_time": "2024-07-02T16:20:46.730384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Apply preprocessing in parallel\n",
    "with ProgressBar():\n",
    "    train_df = preprocess_texts(train_df).persist()\n",
    "    test_df = preprocess_texts(test_df).persist()\n",
    "\n",
    "# Compute the preprocessed text columns    \n",
    "X_train_ddf = train_df[['processed_text']].compute()\n",
    "y_train_ddf = train_df['rating'].compute()\n",
    "X_test_ddf = test_df[['processed_text']].compute()\n",
    "y_test_ddf = test_df['rating'].compute()\n",
    "\n",
    "# Extract processed text columns\n",
    "X_train = X_train_ddf['processed_text']\n",
    "X_test = X_test_ddf['processed_text']\n",
    "\n",
    "# Garbage collection to free memory\n",
    "gc.collect()"
   ],
   "id": "f3faac6783cf8529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 29.80 s\n",
      "[########################################] | 100% Completed | 3.21 ss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86090"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:21:31.306103Z",
     "start_time": "2024-07-02T16:21:27.280672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 5000  # Use the num_words value directly\n",
    "# Initialize tokenizer specific for LSTM\n",
    "tokenizer_lstm = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer_lstm.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer_lstm.texts_to_sequences(X_train)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "\n",
    "X_test_seq = tokenizer_lstm.texts_to_sequences(X_test)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Encode target labels to one-hot\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_ddf)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "y_test = label_encoder.transform(y_test_ddf)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)"
   ],
   "id": "122e2998dc3a6857",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Função para criar modelo LSTM ",
   "id": "d0d57f1e1b21d505"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:35:15.992529Z",
     "start_time": "2024-07-02T16:35:15.983713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para criar modelo LSTM\n",
    "def create_model(dropout_rate= 0.5, LSTM_layers= 1, LSTM_neurons= 64, dense_layers= 1, dense_neurons= 32, activation= 'relu'):\n",
    "    try:\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=128, input_shape=(100,)))\n",
    "        \n",
    "        \n",
    "        if LSTM_layers == 3:\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        if LSTM_layers == 2:\n",
    "            model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)))\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "        \n",
    "        model.add(Bidirectional(LSTM(LSTM_neurons, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=False)))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        for _ in range(dense_layers-1):\n",
    "            model.add(Dense(dense_neurons, activation=activation, kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        \n",
    "        model.add(Dense(dense_neurons, activation=activation, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dense(2, kernel_initializer='uniform', activation='softmax'))  # Adjusted this line\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while adding layers: {e}\")\n",
    "        return None"
   ],
   "id": "3843dc7832b392f3",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Custom Callback   \n",
    "\n",
    "Criamos um custom callback para monitorar o treinamento do modelo e salvar os melhores pesos. De forma que eu posso escolher quantas épocas esperar para reduzir o learning rate e quantas épocas esperar para restaurar os melhores pesos. Então Criei uma Rotina dessa forma:\n",
    "\n",
    "- Se o modelo não melhorar por um número de épocas (patience) ele restaura os melhores pesos.\n",
    "- Se o modelo não melhorar por um número de épocas (lr_patience) ele reduz o learning rate. \n",
    "- Se o learning rate chegar no mínimo, ele para o treinamento.  \n",
    "\n",
    "Dessa forma a estrutura deixa o modelo re-testar o learning_rate no melhor ponto de mínimo local, e não deixa o modelo divagar.\n",
    "se mesmo assim aquele learning_rate não funcionar, ele vai diminuindo até chegar no mínimo e parar o treinamento. \n",
    "\n",
    "Também criei outra rotina onde ela:\n",
    "- salva os pesos e os resultados de 3 épocas que rodam com os mesmo pesos\n",
    "- compara se uma dessas 3 épocas melhorou o modelo.\n",
    "- Se Melhorou atualiza os pesos\n",
    "- Se piorou, ele reduz o learning rate e realiza mais 3 épocas\n",
    "- Se o learning_rate chegar no mínimo encerra-se o treinamento.\n",
    "\n",
    "Infelizmente o Callback ReduceLROnPlateau não funciona dessa forma, ele reduz o learning rate mas ele aplica nos pesos atuais que podem ter divagado muito longe, e não nos melhores pesos. então por exemplo, se com o learning rate de 0.01 o modelo piorar significativamente no periodo de pacience, ele reduzirá o learning rate para 0.004, mas continuará treinando com os pesos atuais que pioraram o modelo e estão em um minimo local pior. Por exemplo: ele no inicio do treinamento encontrou um mínimo local de val_loss de 0.4 7 epócas dentro do treinamento, mas nas outras interações ele divagou para 0.55, o ReduceLROnPlateau vai reduzir o learning rate para 0.001 mas o modelo vai continuar do local val_loss de 0.55 e com um learning_rate menor dificilmente irá achar um mínimo local melhor."
   ],
   "id": "26cd7938c7518d02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Custom Callback   \n",
    "\n",
    "Criamos um custom callback para monitorar o treinamento do modelo e salvar os melhores pesos. De forma que eu posso escolher quantas épocas esperar para reduzir o learning rate e quantas épocas esperar para restaurar os melhores pesos. Então Criei uma Rotina dessa forma:\n",
    "\n",
    "- Se o modelo não melhorar por um número de épocas (patience) ele restaura os melhores pesos.\n",
    "- Se o modelo não melhorar por um número de épocas (lr_patience) ele reduz o learning rate. \n",
    "- Se o learning rate chegar no mínimo, ele para o treinamento.  \n",
    "\n",
    "Dessa forma a estrutura deixa o modelo re-testar o learning_rate no melhor ponto de mínimo local, e não deixa o modelo divagar.\n",
    "se mesmo assim aquele learning_rate não funcionar, ele vai diminuindo até chegar no mínimo e parar o treinamento. \n",
    "\n",
    "Também criei outra rotina onde ela:\n",
    "- salva os pesos e os resultados de 3 épocas que rodam com os mesmo pesos\n",
    "- compara se uma dessas 3 épocas melhorou o modelo.\n",
    "- Se Melhorou atualiza os pesos\n",
    "- Se piorou, ele reduz o learning rate e realiza mais 3 épocas\n",
    "- Se o learning_rate chegar no mínimo encerra-se o treinamento.\n",
    "\n",
    "Infelizmente o Callback ReduceLROnPlateau não funciona dessa forma, ele reduz o learning rate mas ele aplica nos pesos atuais que podem ter divagado muito longe, e não nos melhores pesos. então por exemplo, se com o learning rate de 0.01 o modelo piorar significativamente no periodo de pacience, ele reduzirá o learning rate para 0.004, mas continuará treinando com os pesos atuais que pioraram o modelo e estão em um minimo local pior. Por exemplo: ele no inicio do treinamento encontrou um mínimo local de val_loss de 0.4 7 epócas dentro do treinamento, mas nas outras interações ele divagou para 0.55, o ReduceLROnPlateau vai reduzir o learning rate para 0.001 mas o modelo vai continuar do local val_loss de 0.55 e com um learning_rate menor dificilmente irá achar um mínimo local melhor."
   ],
   "id": "7ae3e7a6bde077ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:35:17.217794Z",
     "start_time": "2024-07-02T16:35:17.206976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, patience=0, lr_patience=3, reduce_factor=0.5, min_lr=0.0001):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.lr_patience = lr_patience \n",
    "        self.wait = 0\n",
    "        self.lr_wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best_loss = np.inf\n",
    "        self.reduce_factor = reduce_factor\n",
    "        self.min_lr = min_lr\n",
    "        self.best_epoch = 0\n",
    "        self.results = []\n",
    "        self.weights = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.lr_wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best_loss = np.inf\n",
    "        self.results = []\n",
    "        self.weights = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        print(\"\\nLearning rate is %6.4f.\" % (lr))\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        # print(\"current wait count:\", self.wait)\n",
    "        print(\"best loss:\", self.best_loss)\n",
    "        print(\"current loss:\", logs.get(\"val_loss\"))\n",
    "        if self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(\"val_loss\")\n",
    "        self.results.append(current_loss)\n",
    "        self.weights.append(self.model.get_weights())\n",
    "        if self.best_weights is None:\n",
    "            self.best_weights = self.model.get_weights()  \n",
    "        if len(self.results) == 3:\n",
    "            min_loss_index = np.argmin(self.results)\n",
    "            min_loss = self.results[min_loss_index]\n",
    "            if min_loss < self.best_loss:\n",
    "                self.best_loss = min_loss\n",
    "                self.best_weights = self.weights[min_loss_index]\n",
    "            else:\n",
    "                old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "                new_lr = old_lr * self.reduce_factor\n",
    "                new_lr = max(new_lr, self.min_lr)\n",
    "                if new_lr == self.min_lr:\n",
    "                    self.model.stop_training = True\n",
    "                self.model.optimizer.learning_rate = tf.Variable(new_lr)\n",
    "            self.results = []\n",
    "            self.weights = []\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     current_loss = logs.get(\"val_loss\")\n",
    "    #     if np.less(current_loss, self.best_loss):\n",
    "    #         self.best_loss = current_loss\n",
    "    #         self.best_weights = self.model.get_weights()\n",
    "    #         self.best_epoch = epoch\n",
    "    #         self.wait = 0\n",
    "    #         self.lr_wait = 0\n",
    "    #     else:\n",
    "    #         self.wait += 1\n",
    "    #         self.lr_wait += 1\n",
    "    #         print(f\"\\nLoss did not improve. Current wait count: {self.wait}.\")\n",
    "    #         if self.wait >= self.patience:\n",
    "    #             print('restoring best weights') \n",
    "    #             self.model.set_weights(self.best_weights)\n",
    "    #             self.wait = 0\n",
    "    #         if self.lr_wait >= self.lr_patience:\n",
    "    #             print('reducing learning rate')\n",
    "    #             old_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "    #             new_lr = old_lr * self.reduce_factor\n",
    "    #             new_lr = max(new_lr, self.min_lr)\n",
    "    #             if new_lr == self.min_lr:\n",
    "    #                 self.model.stop_training = True\n",
    "    #             self.model.optimizer.learning_rate = tf.Variable(new_lr)\n",
    "    #             self.lr_wait = 0\n",
    "                \n",
    "                \n",
    "# Instantiate the custom callback and use it as an argument in model.fit\n",
    "custom_callback = CustomCallback(patience=3,lr_patience=6)"
   ],
   "id": "79608c412b60397",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Treinamento com os melhores parametros encontrados na random search mas com um número maior de épocas.",
   "id": "c60cc37d6d790219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:35:18.485887Z",
     "start_time": "2024-07-02T16:35:18.240817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar modelo com os melhores parametros\n",
    "model_lstm = create_model(dropout_rate=0.5, LSTM_layers=2, LSTM_neurons=64, dense_layers=1, dense_neurons=32, activation='relu')\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Define callbacks\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=1, min_lr=0.0001)\n",
    "# model_checkpoint = ModelCheckpoint('best_model_lstm_checkpoint.keras', monitor='val_loss', save_best_only=True, save_weights_only=False)"
   ],
   "id": "469f649b96a31ee0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucia\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:31:29.078938Z",
     "start_time": "2024-07-02T16:31:29.059019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_lstm.summary()"
   ],
   "id": "5e240733b0dd0d8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_8\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001B[38;5;33mEmbedding\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │       \u001B[38;5;34m640,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_16                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │        \u001B[38;5;34m98,816\u001B[0m │\n",
       "│ (\u001B[38;5;33mBidirectional\u001B[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_17                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m98,816\u001B[0m │\n",
       "│ (\u001B[38;5;33mBidirectional\u001B[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m4,128\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │            \u001B[38;5;34m66\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_16                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_17                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m842,850\u001B[0m (3.22 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,850</span> (3.22 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m842,338\u001B[0m (3.21 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">842,338</span> (3.21 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m512\u001B[0m (2.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_lstm.fit(X_train_pad, y_train,\n",
    "               batch_size=64, epochs=50, validation_data=(X_test_pad, y_test), verbose=2, callbacks=[custom_callback])\n",
    "\n",
    "# Restaurar melhores pesos\n",
    "model_lstm.set_weights(custom_callback.best_weights)\n",
    "\n",
    "# salvar modelo\n",
    "model_lstm.save('best_model_lstm_30_epochs.keras')\n",
    "\n",
    "# Avaliar modelo    \n",
    "y_pred = np.argmax(model_lstm.predict(X_test_pad), axis=-1)\n",
    "y_test_binary = np.argmax(y_test, axis=-1)  # Convert y_test to binary format\n",
    "accuracy_lstm = accuracy_score(y_test_binary, y_pred)  # Use y_test_binary here\n",
    "# classification report\n",
    "report_lstm = classification_report(y_test_binary, y_pred, zero_division= 0)  # Use y_test_binary here\n"
   ],
   "id": "5fff0c2e472ca6a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T04:00:29.730480Z",
     "start_time": "2024-07-02T04:00:29.726990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Accuracy:', accuracy_lstm)       \n",
    "print(report_lstm)\n"
   ],
   "id": "558799b90f4f73c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      4025\n",
      "           1       0.85      0.86      0.85      3975\n",
      "\n",
      "    accuracy                           0.86      8000\n",
      "   macro avg       0.86      0.86      0.86      8000\n",
      "weighted avg       0.86      0.86      0.86      8000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6ab021c1cba85607"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
