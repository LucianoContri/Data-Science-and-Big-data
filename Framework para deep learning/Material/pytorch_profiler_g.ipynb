{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**torch.profiler**\n",
        "\n",
        "O pacote torch.profiler no PyTorch é uma ferramenta que permite a coleta de métricas de desempenho durante o treinamento e a inferência de modelos de aprendizado de máquina. Com a API de gerenciamento de contexto do torch.profiler, você pode entender quais operadores do modelo são os mais custosos, examinar suas formas de entrada e rastreamentos de pilha, estudar a atividade do kernel do dispositivo e visualizar o rastro de execução"
      ],
      "metadata": {
        "id": "qd1iIMi7-yIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn5NXC3zhiWh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch_tb_profiler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ],
      "metadata": {
        "id": "XFB5DcHZhn2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Criando o modelo ResNet e gerando dados aleatórios**"
      ],
      "metadata": {
        "id": "gYwvJkbZGnuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5, 3, 224, 224)"
      ],
      "metadata": {
        "id": "v3eUe6uBhqPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fazendo uma predição com base nos dados aleatórios e acompanhando a execução de forma simples**"
      ],
      "metadata": {
        "id": "rG0WbeUoGs6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(activities=[ProfilerActivity.CPU],\n",
        "             record_shapes=True) as model_profile:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ],
      "metadata": {
        "id": "OUDV4pROh29y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**imprimindo os resultados**"
      ],
      "metadata": {
        "id": "AgPUN7OiG118"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_profile.key_averages().table())"
      ],
      "metadata": {
        "id": "3dY87uO5nsjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_profile.key_averages().table(sort_by=\"cpu_time_total\",\n",
        "                                         row_limit=10))"
      ],
      "metadata": {
        "id": "ekunPBuTh55H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_profile.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\",\n",
        "                                                                  row_limit=10))"
      ],
      "metadata": {
        "id": "V1WqEhzOh9EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Acompanhando execução em CPU e GPU**"
      ],
      "metadata": {
        "id": "HyDZPwM3BUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma maneira de usar o torch.profiler para coletar informações detalhadas sobre o desempenho de um modelo PyTorch durante a inferência\n",
        "\n",
        "- with profile(...) as model_profile: Este bloco de código inicia o contexto do profiler, especificando quais atividades devem ser monitoradas. Neste caso, estamos monitorando atividades na CPU e na CUDA (GPU).\n",
        "- activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]: Define as atividades que serão perfiladas. ProfilerActivity.CPU monitora operações na CPU, enquanto ProfilerActivity.CUDA monitora operações na GPU.\n",
        "- record_shapes=True: Indica que o profiler deve registrar as formas (shapes) dos tensores que são passados para as operações.\n",
        "- as model_profile: Este é o objeto de perfil que será usado para acessar os dados coletados após a execução do bloco.\n",
        "- with record_function(\"model_inference\"): Um contexto que permite rotular um bloco de código específico, neste caso, a inferência do modelo. Isso ajuda a identificar e separar as métricas de desempenho para essa parte específica do código.\n",
        "- model(inputs): É a chamada ao modelo com os dados de entrada, que é o código que você deseja perfilar.\n",
        "\n",
        "Após a execução desse bloco de código, o objeto model_profile conterá os dados de desempenho coletados, que possasmos analisar para entender onde estão os gargalos de desempenho e como otimizar o modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "cs-QiVUKL37P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[\n",
        "        ProfilerActivity.CPU,\n",
        "        ProfilerActivity.CUDA\n",
        "        ],\n",
        "             record_shapes=True) as model_profile:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)\n",
        "\n",
        "print(model_profile.key_averages().table(sort_by=\"cuda_time_total\",\n",
        "                                         row_limit=10))"
      ],
      "metadata": {
        "id": "DTFMjWlaiC60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5, 3, 224, 224)\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU],\n",
        "             profile_memory=True, #rastreia alocação e liberação de memória\n",
        "             record_shapes=True) as model_profile:\n",
        "    model(inputs)"
      ],
      "metadata": {
        "id": "x1PKNw_FiF_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_profile.key_averages().table(sort_by=\"cpu_memory_usage\",\n",
        "                                         row_limit=10))"
      ],
      "metadata": {
        "id": "u12KEZkAiJ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU,\n",
        "                         ProfilerActivity.CUDA]) as model_profile:\n",
        "    model(inputs)\n",
        "\n",
        "model_profile.export_chrome_trace(\"trace.json\")"
      ],
      "metadata": {
        "id": "VrRf0sjfiKbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU,\n",
        "                ProfilerActivity.CUDA],\n",
        "    with_stack=True, #O parâmetro with_stack=True é usado no contexto do torch.profiler d\n",
        "                     #o PyTorch para incluir informações de rastreamento de pilha nos dados de perfil.\n",
        "                     #Quando ativado, ele permite que o profiler registre a pilha de chamadas que levou\n",
        "                     #a cada operação registrada. Isso é útil para identificar exatamente onde no código\n",
        "                     #fonte as operações mais custosas estão sendo chamadas1\n",
        ") as model_profile:\n",
        "    model(inputs)\n",
        "\n",
        "print(model_profile.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
      ],
      "metadata": {
        "id": "xwpYXzm8iNlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_profile.export_stacks(\"/tmp/profiler_stacks.txt\",\n",
        "                            \"self_cuda_time_total\")"
      ],
      "metadata": {
        "id": "j3XrXZFAiQ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/tmp')"
      ],
      "metadata": {
        "id": "bPJGkP7SNRp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Adicionando um Schedule para customizar a coleta de dados**\n",
        "\n",
        "- from torch.profiler import schedule: Esta linha importa a função schedule do módulo torch.profiler.\n",
        "- my_schedule = schedule(...): Aqui, my_schedule é definido como uma chamada à função schedule, que retorna um objeto de agendamento configurável.\n",
        "- skip_first=10: O profiler irá ignorar as primeiras 10 etapas (ou iterações) antes de começar qualquer coleta de dados. Isso pode ser útil para evitar a medição de tempos de inicialização atípicos.\n",
        "- wait=5: Após pular as etapas iniciais, o profiler irá esperar por mais 5 etapas sem coletar dados. Isso permite que o modelo estabilize e reduz o impacto do overhead de inicialização.\n",
        "- warmup=1: Durante a fase de aquecimento, que dura 1 etapa, o profiler começa a preparar-se para a coleta de dados, mas ainda não coleta métricas ativamente.\n",
        "- active=3: Após o aquecimento, o profiler ficará ativo e coletará dados por 3 etapas.\n",
        "- repeat=2: Todo o ciclo de wait, warmup e active será repetido 2 vezes."
      ],
      "metadata": {
        "id": "GEc5raRMF8gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import schedule\n",
        "\n",
        "my_schedule = schedule(\n",
        "    skip_first=10,\n",
        "    wait=5,\n",
        "    warmup=1,\n",
        "    active=3,\n",
        "    repeat=2)"
      ],
      "metadata": {
        "id": "hK_99LQJiTcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função trace_handler é um manipulador de rastreamento personalizado usado com o torch.profiler para processar os dados de perfil coletados durante a execução do modelo.\n",
        "\n",
        "- model_profile: Este é o objeto que contém os dados de perfil coletados pelo torch.profiler.\n",
        "- model_profile.key_averages(): Calcula as médias das métricas de desempenho para cada operador ou evento que foi rastreado.\n",
        ".table(sort_by=\"self_cuda_time_total\", row_limit=10): Gera uma tabela formatada das médias, ordenando os resultados pelo tempo total gasto em operações CUDA (self_cuda_time_total) e limitando a saída às 10 linhas superiores.\n",
        "- print(output): Imprime a tabela formatada no console, permitindo que você veja rapidamente quais operadores ou eventos estão consumindo mais tempo na GPU.\n",
        "- model_profile.export_chrome_trace(...): Exporta os dados de perfil para um arquivo JSON que pode ser visualizado usando o chrome://tracing no navegador Google Chrome. O nome do arquivo inclui o número da etapa de perfil (step_num), que ajuda a identificar e organizar os arquivos de rastreamento se você estiver coletando vários ao longo do tempo.\n"
      ],
      "metadata": {
        "id": "8fXWXZpsJ5BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_handler(model_profile):\n",
        "    output = model_profile.key_averages().table(sort_by=\"self_cuda_time_total\",\n",
        "                                                row_limit=10)\n",
        "    print(output)\n",
        "    model_profile.export_chrome_trace(\"/tmp/trace_\" + str(model_profile.step_num) + \".json\")"
      ],
      "metadata": {
        "id": "cKsiVoPqGSGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(\n",
        "        wait=1,\n",
        "        warmup=1,\n",
        "        active=2),\n",
        "    on_trace_ready=trace_handler\n",
        ") as model_profile:\n",
        "    for idx in range(8):\n",
        "        model(inputs)\n",
        "        model_profile.step()"
      ],
      "metadata": {
        "id": "Y5BNgWYiiV2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Aplicando o torch.profiler em alguns passos do treino da ResNet**"
      ],
      "metadata": {
        "id": "Wr8MQxltDKGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "model = torchvision.models.resnet18(weights='IMAGENET1K_V1').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "bEft9yd9l-40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fazendo os imports**"
      ],
      "metadata": {
        "id": "5JySo0ClDUxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torch.profiler\n",
        "import torch.utils.data\n",
        "import torchvision.datasets\n",
        "import torchvision.models\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "OsTzCAlcmWy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Criando o processamento do dado dataset e dataloader**"
      ],
      "metadata": {
        "id": "ZPOOjhK0DZhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose(\n",
        "    [T.Resize(224),\n",
        "     T.ToTensor(),\n",
        "     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                         train=True,\n",
        "                                         download=True,\n",
        "                                         transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True)"
      ],
      "metadata": {
        "id": "MQ35UkJ4mTUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Criando a função que executa um passo no treinamento**"
      ],
      "metadata": {
        "id": "_dw0KxaDDgGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(data):\n",
        "    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "94J8TxUBiaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.profiler.schedule(wait=1,\n",
        "                                   warmup=1,\n",
        "                                   active=3,\n",
        "                                   repeat=1)"
      ],
      "metadata": {
        "id": "HN0_F4emE3OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.profiler.profile(\n",
        "        schedule=scheduler,\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/resnet18'),\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True\n",
        ") as model_profile:\n",
        "\n",
        "    for step, batch_data in enumerate(train_loader):\n",
        "        model_profile.step()\n",
        "        if step == 10:\n",
        "            break\n",
        "        train_step(batch_data)"
      ],
      "metadata": {
        "id": "f4SaBgVbmP67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./log/resnet18"
      ],
      "metadata": {
        "id": "CHWLRH7umgCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}